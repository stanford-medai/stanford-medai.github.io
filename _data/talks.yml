# - speaker:
#   type: upcoming (or previous)
#   date:
#   title:
#   abstract:
#   bio:
#   livestream:
#   recording:

#### Previous talks: make sure order is right. Can just replace upcoming with previous in place

- speaker: Amirata Ghorbani
  institution: Stanford University
  type: previous
  date: 4/1/21
  title: Equitable Valuation of Data
  abstract: As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this talk, we discuss a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on a number of data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We introduce Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. We then briefly discuss the notion of distributional Shapley, where the value of a point is defined in the context of underlying data distribution.
  bio: <a href="https://www.amiratag.com/" target="_blank">Amirata Ghorbani</a> is a fifth year PhD student at Stanford University advised by James Zou. He primarily works on different problems in machine learning such as research on equitable methods for data valuation, algorithms to interpret machine learning models, ways to make existing ML predictors fairer, and creating ML systems for healthcare applications such as cardiology and dermatology. He has also worked as a research intern in Google Brain, Google Brain Medical, and Salesforce Research.
  recording: https://youtu.be/TzbFPsJ3o7U

- speaker: Michael Zhang
  institution: Stanford University
  type: previous
  date: 4/8/21
  title: Federated Learning with FOMO for Personalized Training and Deployment
  abstract: Federated learning (FL) is an exciting and relatively new deep learning framework that canonically trains a single global model across decentralized local datasets maintained by participating clients. Accordingly with respect to making deep learning more deployable, FL is particularly promising in real world settings where technological or privacy constraints prevent individual data from being aggregated together. However, one model may not always be optimal for all participating clients. From healthcare to recommendation systems, we would ideally like to learn and deliver a personalized model for each participating client, as data may not be identically distributed from one client to another. This problem is emphasized when we consider how we might deploy FL in practice, where individual clients may only choose to federate if they can guarantee a benefit from the model produced at the end.<br><br>In this talk, I will present some recent work on one solution, called FedFomo, where each client effectively only federates with other relevant clients to obtain stronger personalization. First we will review federated learning as a machine learning framework, emphasizing the motivations behind personalized FL. I will then go into the origin story of FedFomo's name, highlighting a simple yet effective approach based both on the "fear of missing out" and "first order model optimization". In tandem, these ideas describe how FedFomo can efficiently figure out how much each client can benefit from another's locally trained model, and then use these values to calculate optimal federated models for each client. Critically, this does not assume knowledge of any underlying data distributions or client similarities, as this information is often not known apriori. Finally, I will describe recent empirical results on FedFomo's promising performance on a variety of federated settings, datasets, and degrees of local data heterogeneity, leading to wider discussion on the future directions and impact of federated learning and distributed machine learning, when personalization is in the picture.
  bio: Michael Zhang is a Computer Science PhD Student at Stanford, currently working with Chris Ré and Chelsea Finn. He is broadly interested in making machine learning more deployable and reliable in the "real world", especially through the lenses of improving model robustness and personalization to distribution shifts and new tasks, as well as developing new systems that enable collaborative machine learning and/or learning with less labels.
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  recording: https://www.youtube.com/watch?v=svBDaT4IY4A

- speaker: Jason Fries
  institution: Stanford University
  type: previous
  date: 4/15/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Weakly Supervised Learning in Medicine (Better Living through Programmatic Supervision)
  abstract: The high cost of building labeled training sets is one of the largest barriers to using supervised machine learning in medicine. Privacy concerns create additional challenges to sharing training data for modalities like patient notes, making it difficult to train state-of-the-art NLP tools for analyzing electronic health records. The COVID-19 pandemic underscores the need for faster, more systematic methods of curating and sharing training data. One promising approach is weakly supervised learning, where low cost and often noisy label sources are combined to programmatically generate labeled training data for commodity deep learning architectures such as BERT. Programmatic labeling takes a data-centric view of machine learning and provides many of the same practical benefits as software development, including better consistency, inspectability, and creating higher-level abstractions for experts to inject domain knowledge into machine learning models.<br><br>In this talk I outline our new framework for weakly supervised clinical entity recognition, Trove, which builds training data by combining multiple public medical ontologies and other imperfect label sources. Instead of manually labeling data, in Trove annotators focus on defining labelers using ontology-based properties like semantic types as well as optional task-specific rules. On four named entity benchmark tasks, Trove approaches the performance of models trained using hand-labeled data. However unlike hand-labeled data, our labelers can be shared and modified without compromising patient privacy.
  bio: Jason Fries (<a href="http://web.stanford.edu/~jfries/" target="_blank">http://web.stanford.edu/~jfries/</a>) is a Research Scientist at Stanford University working with Professor Nigam Shah at the Center for Biomedical Informatics Research. He previously completed his postdoc with Professors Chris Ré and Scott Delp as part of Stanford's Mobilize Center. He received his PhD in computer science from the University of Iowa, where he studied computational epidemiology and NLP methods for syndromic surveillance. His recent research explores weakly supervised and few-shot learning in medicine, with a focus on methods for incorporating domain knowledge into the training of machine learning models. 
  recording: https://www.youtube.com/watch?v=EcvLqmeD7SE

- speaker: Pradeeban Kathiravelu
  institution: Emory University
  type: previous
  date: 4/22/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Understanding Scanner Utilization with Real-Time DICOM Metadata Extraction
  abstract: Understanding system performance metrics ensures better utilization of the radiology resources with more targeted interventions. The images produced by radiology scanners typically follow the DICOM (Digital Imaging and Communications in Medicine) standard format. The DICOM images consist of textual metadata that can be used to calculate key timing parameters, such as the exact study durations and scanner utilization. However, hospital networks lack the resources and capabilities to extract the metadata from the images quickly and automatically compute the scanner utilization properties. Thus, they resort to using data records from the Radiology Information Systems (RIS). However, data acquired from RIS are prone to human errors, rendering many derived key performance metrics inadequate and inaccurate. Hence, there is motivation to establish a real-time image transfer from the Picture Archiving and Communication Systems (PACS) to receive the DICOM images from the scanners to research clusters to conduct such metadata processing to evaluate scanner utilization metrics efficiently and quickly.<br><br>In this talk, we present Niffler (https://github.com/Emory-HITI/Niffler), an open-source DICOM Framework for Machine Learning Pipelines and Processing Workflows. Niffler analyzes the scanners' utilization as a real-time monitoring framework that retrieves radiology images into a research cluster using the DICOM networking protocol and then extracts and processes the metadata from the images. Niffler facilitates a better understanding of scanner utilization across a vast healthcare network by observing properties such as study duration, the interval between the encounters, and the series count of studies. Benchmarks against using the RIS data indicate that our proposed framework based on real-time PACS data estimates the scanner utilization more accurately. Our framework has been running stable and supporting several machine learning workflows for more than two years on our extensive healthcare network in pseudo-real-time. We further present how we use the Niffler framework for real-time and on-demand execution of machine learning (ML) pipelines on radiology images.
  bio: Pradeeban Kathiravelu is a postdoctoral researcher at the Department of Biomedical Informatics in Emory University. He has an Erasmus Mundus Joint Doctorate in Distributed Computing from Universidade de Lisboa (Lisbon, Portugal) and Université catholique de Louvain (Louvain-la-Neuve, Belgium). His research focus includes researching and developing latency-aware Software-Defined Systems and cloud-assisted networks for radiology workflows at the edge.
  recording: https://youtu.be/oiQAjfYc9Sc


- speaker: Joseph Cohen
  institution: Stanford University
  type: previous
  date: 4/29/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Gifsplanation via Latent Shift - A Simple Autoencoder Approach to Counterfactual Generation for Chest X-rays
  abstract: <strong>Motivation</strong> Traditional image attribution methods struggle to satisfactorily explain predictions of neural networks. Prediction explanation is important, especially in medical imaging, for avoiding the unintended consequences of deploying AI systems when false positive predictions can impact patient care. Thus, there is a pressing need to develop improved models for model explainability and introspection. <br><strong>Specific problem</strong> A new approach is to transform input images to increase or decrease features which cause the prediction. However, current approaches are difficult to implement as they are monolithic or rely on GANs. These hurdles prevent wide adoption.<br><strong>Our approach</strong> Given an arbitrary classifier, we propose a simple autoencoder and gradient update (Latent Shift) that can transform the latent representation of a specific input image to exaggerate or curtail the features used for prediction. We use this method to study chest X-ray classifiers and evaluate their performance. We conduct a reader study with two radiologists assessing 240 chest X-ray predictions to identify which ones are false positives (half are) using traditional attribution maps or our proposed method. This work will be presented at MIDL 2021.<br><strong>Results</strong> We found low overlap with ground truth pathology masks for models with reasonably high accuracy. However, the results from our reader study indicate that these models are generally looking at the correct features.We also found that the Latent Shift explanation allows a user to have more confidence in true positive predictions compared to traditional approaches (0.15±0.95 in a 5 point scale with p=0.01) with only a small increase in false positive predictions (0.04±1.06 with p=0.57).<br><strong>Project Page</strong> https://mlmed.org/gifsplanation/ <br><strong>Source code</strong> https://github.com/mlmed/gifsplanation
  bio: Joseph Paul Cohen is a researcher and pragmatic engineer. He currently focuses on the challenges in deploying AI tools in medicine specifically computer vision and genomics and is affiliated to Stanford AIMI. He maintains many open source projects including Chester the AI radiology assistant, TorchXRayVision, and BlindTool – a mobile vision aid app. He is the director of the Institute for Reproducible Research, a US non-profit which operates ShortScience.org and Academic Torrents.
  recording: https://youtu.be/2dHY5IJTbKM
  
- speaker: Angshuman Paul
  institution: NIH
  type: previous
  date: 5/6/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Few-shot Chest X-ray Diagnosis Using Clinical Images and the Images from the Published Scientific Literature
  abstract: Few-shot learning is the art of machine learning that tries to mimic the human cognitive ability of understanding new object classes from a few labeled training examples. In the last few years, several few-shot learning methods have been proposed for different tasks related to natural images. However, few-shot learning is relatively unexplored in the field radiology image analysis. In this seminar, we will present two few-shot learning methods for chest x-ray diagnosis. Our first method uses a discriminative ensemble trained using labeled clinical chest x-ray images. The second method uses labeled chest x-ray images from the published scientific literature and unlabeled clinical chest x-ray images to train a machine learning model. Experiments show the superiority of the proposed methods over several existing few-shot learning methods.
  bio: Angshuman Paul (M.E., Ph.D.) is a visiting (postdoctoral) fellow at the National Institutes of Health, USA. His primary research interest is in Machine Learning, Medical Imaging, and Computer Vision. He earned his Ph.D. from the Indian Statistical Institute, India. He has held a visiting scientist position at the Indian statistical Institute (2019) and a graduate intern position at the University of Missouri-Columbia (2011). Dr. Paul is the recipient of the NIH Intramural Fellowship (2019) from the National Institutes of Health, USA, and the best paper award in the Tenth Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP, 2016). He serves as a reviewer of several journals including IEEE Transactions on Medical Imaging, Pattern Recognition Letters, and IEEE Transactions on Image Processing.
  recording: https://www.youtube.com/watch?v=TtXrowIpXPM

- speaker: Xiaoyuan Guo
  institution: Emory University
  type: previous
  date: 5/13/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Segmentation and Quantification of Breast Arterial Calcifications (BAC) on Mammograms
  abstract: Measurements of breast arterial calcifications (BAC) can offer a personalized, noninvasive approach to risk-stratify women for cardiovascular disease such as heart attack and stroke. We aim to detect and segment breast arterial calcifications in mammograms accurately and suggest novel measurements to quantify detected BAC for future clinical applications. To separate BAC in mammograms, we propose a lightweight fine vessel segmentation method Simple Context U-Net (SCU-Net). To further quantify calcifications, we test five quantitative metrics to inspect the progression of BAC for subjects- Sum of Mask Probability Metric (PM), Sum of Mask Area Metric (AM), Sum of Mask Intensity Metric (SIM), Sum of Mask Area with Threshold Intensity Metric (TAMx) and Sum of Mask Intensity with Threshold X Metric (TSIMx). Finally, we demonstrate the ability of the metrics to longitudinally measure calcifications in a group of 26 subjects and evaluate our quantification metrics compared to calcified voxels and calcium mass on breast CT for 10 subjects.
  bio: Xiaoyuan Guo is a Computer Science PhD student at Emory University, working with Prof. Imon Banerjee, Prof. Hari Trivedi and Prof. Judy Wawira Gichoya. Her primary research interests are computer vision and medical image processing, especially improving medical image segmentation, classification, object detection accuracy with mainstream computer vision techniques. She is also interested in solving open-world medical tasks.
  recording: https://www.youtube.com/watch?v=CWuBD9XTN7s

- speaker: Nandita Bhaskhar
  institution: Stanford University
  type: previous
  date: 5/20/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Self-supervision & Contrastive Frameworks -- a vision-based review
  abstract: Self-supervised representation learning and contrastive techniques have picked up a lot interest in the last couple of years, especially in computer vision. Until recently, deep learning's successes thus far have been associated with a supervised learning paradigm, wherein labelled datasets are used to train models on specific tasks. This need for labelled datasets has been identified as the bottleneck for scaling deep learning models across various tasks and domains. They rely heavily on costly, time-consuming dataset curation and labelling schemes. <br><br>Self-supervision allows us to learn representations from large unlabelled datasets. Instead of relying on labels for inputs, it depends on designing suitable pre-text tasks to generate pseudo-labels from the data directly. Contrastive learning refers to a special subset of these self-supervised methods that have achieved the most success recently. In this talk, I will go over the top 6 recent frameworks - SimCLR, MoCo V2, BYOL, SwAV, DINO and Barlow Twins, giving a deeper dive into their methodology & performance and comparing each of the frameworks' strengths and weaknesses and discuss their suitability for applications in the medical domain.
  bio: Nandita Bhaskhar (see <a href="http://web.stanford.edu/~nanbhas/" target="_blank">website</a>) is a PhD student in the Department of Electrical Engineering at Stanford University advised by Daniel Rubin. She received her B.Tech in Electronics Engineering from the Indian Institute of Information Technology, IIIT, with the highest honours. She is broadly interested in developing machine learning methodology for medical applications. Her current research focuses on observational supervision and self-supervision for leveraging unlabelled medical data and out-of-distribution detection for reliable clinical deployment. Outside of research, her curiosity lies in a wide gamut of things including but not restricted to biking, social dance, travelling, creative writing, music, getting lost, hiking and exploring new things.
  recording: https://youtu.be/OF_7dBbb_N0

- speaker: Amara Tariq
  institution: Emory University
  type: previous
  date: 5/27/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Patient-specific COVID-19 Resource Utilization Prediction Using Fusion AI model
  abstract: Strain on healthcare resources brought forth by recent COVID-19 pandemic has highlighted the need for efficient resource planning and allocation through prediction of future consumption. Machine learning can predict resource utilization such as the need for hospitalization based on past medical data stored in electronic medical records (EMR). We experimented with fusion modeling to develop patient-specific clinical event prediction model based on patient’s medical history and current medical indicators. A review of feature importance provides insight for future research and feedback from the community on the significance of various predictors of COVID-19 disease trajectory.
  bio: Dr. Amara Tariq received her PhD degree in Computer Science from University of Central Florida in 2016 where she was Fulbright Scholar. Her research was focused on automatic understanding of cross-modal semantic relationships, especially relations between images and text. After earning her PhD, she designed and taught courses focused on Artificial Intelligence and Machine Learning at graduate and post-graduate level in her home country, i.e., Pakistan. Her research interests evolved to include multi-modal data related to the fields of bioinformatics and health science. Since the beginning of 2020, she has been working in post-doctoral research capacity at Bioinformatics department, Emory University, GA. At Emory University, her research has been focused on analyzing electronic medical records, imaging studies, and clinical reports and notes for intelligent decision making regarding disease management and healthcare resource optimization. Her research has resulted in publications in top-tier venues including IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), IEEE Transactions on Image Processing (TIP), IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), Journal of American College of Radiology (JACR), and npj Digital Medicine.
  recording: https://www.youtube.com/watch?v=E4TuOcY89E8
  
- speaker: No session this week -- summer break! 
  type: previous
  date: 6/3/21
  
- speaker: Edward Choi
  institution: KAIST
  type: previous
  date: 6/10/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Learning the Structure of EHR with Graph Convolutional Transformer
  abstract: Large-scale electronic health records (EHR) provide a great opportunity for learning representation of clinical entities (such as codes, visits, patients). As EHR data are typically stored in a relational database, their diverse information (diagnosis, medications, etc) can be naturally viewed as a graph. In this talk, we will study how this graphical structure can be exploited, or even learned for supervised prediction tasks using a combination of graph convolution and self-attention. Additionally, we will briefly present more recent works regarding multi-modal learning using Transformers.
  bio: Edward Choi is currently an assistant professor at KAIST, South Korea. He received his PhD at Georgia Tech under the supervision of Professor Jimeng Sun, focusing on interpretable deep learning methods for longitudinal electronic health records. Before he joined KAIST, Ed was a software engineer at Google Health Research, developing deep learning models for predictive healthcare. His current research interests include machine learning, healthcare analytics, and natural language processing.
  recording: https://www.youtube.com/watch?v=IZXcuWFIBcY&t=18s

- speaker: Florian Dubost
  institution: Stanford University
  type: previous
  date: 6/17/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Hydranet -- Data Augmentation for Regression Neural Networks
  abstract: Deep learning techniques are often criticized to heavily depend on a large quantity of labeled data. This problem is even more challenging in medical image analysis where the annotator expertise is often scarce. We propose a novel data-augmentation method to regularize neural network regressors that learn from a single global label per image. The principle of the method is to create new samples by recombining existing ones. We demonstrate the performance of our algorithm on two tasks- estimation of the number of enlarged perivascular spaces in the basal ganglia, and estimation of white matter hyperintensities volume. We show that the proposed method improves the performance over more basic data augmentation. The proposed method reached an intraclass correlation coefficient between ground truth and network predictions of 0.73 on the first task and 0.84 on the second task, only using between 25 and 30 scans with a single global label per scan for training. With the same number of training scans, more conventional data augmentation methods could only reach intraclass correlation coefficients of 0.68 on the first task, and 0.79 on the second task.
  bio: Florian Dubost is a postdoctoral researcher in biomedical data science at Stanford University, CA, USA, and has with six years of experience in machine learning. He holds a PhD in medical computer vision and reached top rankings in international deep learning competitions. He is member of program committees at conference workshops in AI and medicine, authored a book in AI and neurology, and is an author and reviewer for top international journals and conferences in AI and medicine with over 20 published articles, including 11 as first author.
  recording: https://www.youtube.com/watch?v=ph6xEFAu9A0
  
- speaker: Garrett Honke
  institution: Google X, the Moonshot Factory
  type: previous
  date: 6/24/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: βVAE Representation Learning and Explainability for Psychopathology with EEG and the constraints on deployment in the real world
  abstract: Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using β-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol-Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model.
  bio: Garrett is a neuroscientist working as a Senior Research Scientist at X, the Moonshot Factory (formerly Google X). He works with projects in the early pipeline on problem areas that generally involve ML, datascience, and human-emitted data.
  recording: https://www.youtube.com/watch?v=NU6YZVv0q2Y
  
- speaker: Rikiya Yamashita
  institution: Stanford University
  type: previous
  date: 7/1/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation
  abstract: Suboptimal generalization of machine learning models on unseen data is a key challenge which hampers the clinical applicability of such models to medical imaging. Although various methods such as domain adaptation and domain generalization have evolved to combat this challenge, learning robust and generalizable representations is core to medical image understanding, and continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation for histoPathology), a form of data augmentation based on random style transfer from non-medical style source such as artistic paintings, for learning domain-agnostic visual representations in computational pathology. Style transfer replaces the low-level texture content of an image with the uninformative style of randomly selected style source image, while preserving the original high-level semantic content. This improves robustness to domain shift and can be used as a simple yet powerful tool for learning domain-agnostic representations. We demonstrate that STRAP leads to state-of-the-art performance, particularly in the presence of domain shifts, on two particular classification tasks in computational pathology.
  bio: Rikiya Yamashita is radiologist turned applied research scientist working as a postdoctoral researcher in the Department of Biomedical Data Science at Stanford University. He is broadly interested in developing machine learning methodology for extracting knowledge from unstructured biomedical data. With his dual expertise, he is passionate about bridging the gap between machine learning and clinical medicine.
  recording: https://www.youtube.com/watch?v=WWlGOreDbzU
  
- speaker: Andre Esteva
  institution: Salesforce Research
  type: previous
  date: 7/8/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Frontiers of Medical AI - Therapeutics and Workflows
  abstract: As the artificial intelligence and deep learning revolutions have swept over a number of industries, medicine has stood out as a prime area for beneficial innovation. The maturation of key areas of AI - computer vision, natural language processing, etc. - have led to their successive adoption in certain application areas of medicine. The field has seen thousands of researchers and companies begin pioneering new and creative ways of benefiting healthcare with AI. Here we'll discuss two vitally important areas - therapeutics, and workflows. In the space of therapeutics we'll discuss how multi-modal AI can support physicians in complex decision making for cancer treatments, and how natural language processing can be repurposed to create custom-generated proteins as potential therapeutics. Within workflows, we'll explore how to build a COVID-specialized search engine, and discuss ways in which this could empower health systems to securely, and accurately, search over their highly sensitive data. 
  bio: Andre Esteva is a researcher and entrepreneur in deep learning and computer vision. He currently serves as Head of Medical AI at Salesforce Research. Notably, he has led research efforts in AI-enabled medical diagnostics, and therapeutic decision making. His work has shown that computer vision algorithms can match and exceed the performance of top physicians at diagnosing cancers from medical imagery. Expanded into video they can diagnose behavioral conditions like autism. In the space of AI-enabled therapeutics, his research leverages multi-modal datasets to train AI models that can personalize oncology treatments for patients by determing the best course of therapy for them. He has worked at Google Research, Sandia National Labs, and GE Healthcare, and has co-founded two tech startups. He obtained his PhD in Artificial Intelligence at Stanford, where he worked with Sebastian Thrun, Jeff Dean, Stephen Boyd, Fei-Fei Li, and Eric Topol.
  recording: https://youtu.be/fSpwx8xJ4v8
  

- speaker: No session this week -- Feedback Form <a href="https://forms.gle/EUuvCBdr73ksH3M46">here</a>
  type: previous
  date: 7/15/21
  
- speaker: Liangqiong Qu
  institution: Stanford University
  type: previous
  date: 7/22/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning
  abstract: Federated learning is an emerging research paradigm enabling collaborative training of machine learning models among different organizations while keeping data private at each institution. Despite recent progress, there remain fundamental challenges such as lack of convergence and potential for catastrophic forgetting in federated learning across real-world heterogeneous devices. While most research efforts focus on improving the optimization process in FL, in this talk, we will provide a new perspective by rethinking the choice of architectures in federated models. We will show that simply replacing convolutional networks with Transformers can greatly reduce catastrophic forgetting of previous devices, accelerate convergence, and reach a better global model, especially when dealing with heterogeneous data. The code related to this talk is released <a href="https://github.com/Liangqiong/ViT-FL-main">here</a>, to encourage future exploration in robust architectures as an alternative to current research efforts on the optimization front.
  bio: Liangqiong Qu is currently a postdoctoral researcher at Stanford University. She received her joint-PhD in Pattern Recognition and Intelligent System from University of Chinese Academy of Sciences (2017) and Computer Science from City University of Hong Kong (2017). Before Join Stanford, she was a postdoctoral researcher at IDEA lab in the University of North Carolina at Chapel Hill during 2018~2019.  She has published over 20 peer-reviewed articles including top-tier venues such as CVPR, MedIA, TIP, and MICCAI, and she also wrote a book chapter in Big Data in Psychiatry and Neurology.  
  recording: https://www.youtube.com/watch?v=Ae1CDi0_Nok
  
- speaker: Shantanu Thakoor
  institution: DeepMind
  type: previous
  date: 7/29/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Bootstrapped Self-Supervised Representation Learning in Graphs
  abstract: Self-supervised graph representation learning aims to construct meaningful representations of graph-structured data in the absence of labels. Current state-of-the-art methods are based on contrastive learning, and depend heavily on the construction of augmentations and negative examples. Achieving peak performance requires computation quadratic in the number of nodes, which can be prohibitively expensive. In this talk, we will present Bootstrapped Graph Latents (BGRL) a method for self-supervised graph representation learning that gets rid of this potentially quadratic bottleneck. We show that BGRL outperforms or matches previous methods on several established benchmark datasets, while consuming 2-10x less memory. Moreover, it enables the effective usage of more expressive GNN architectures, allowing us to further improve the state of the art. Finally, we will present our recent results on applying BGRL to the very large-scale data regime, in the OGB-LSC KDD Cup, where it was key to our entry being among the top 3 awardees our track.
  bio: Shantanu is a Research Engineer working at DeepMind. His primary research interests are in graph representation learning and reinforcement learning. Prior to this, he received his MS from Stanford University, where he was working on AI safety and neural network verification, and  B.Tech. from IIT Bombay, where he worked on program synthesis. Recently, he has been interested in applying graph representation learning methods to large-scale problems, including the OGB Large Scale Challenge.
  recording: https://www.youtube.com/watch?v=OkhUCbpWxwc
  
- speaker: Mayee Chen
  institution: Stanford University
  type: previous
  date: 8/5/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Mandoline - Model Evaluation under Distribution Shift
  abstract: Machine learning models are often deployed in different settings than they were trained and validated on, posing a challenge to practitioners who wish to predict how well the deployed model will perform on a target distribution. If an unlabeled sample from the target distribution is available, along with a labeled sample from a possibly different source distribution, standard approaches such as importance weighting can be applied to estimate performance on the target. However, importance weighting struggles when the source and target distributions have non-overlapping support or are high-dimensional. Taking inspiration from fields such as epidemiology and polling, we develop Mandoline, a new evaluation framework that mitigates these issues. Our key insight is that practitioners may have prior knowledge about the ways in which the distribution shifts, which we can use to better guide the importance weighting procedure. Specifically, users write simple "slicing functions" - noisy, potentially correlated binary functions intended to capture possible axes of distribution shift - to compute reweighted performance estimates. We further describe a density ratio estimation framework for the slices and show how its estimation error scales with slice quality and dataset size. Empirical validation on NLP and vision tasks shows that Mandoline can estimate performance on the target distribution up to 3x more accurately compared to standard baselines.<br>This is joint work done with equal contribution from Karan Goel and Nimit Sohoni, as well as Fait Poms, Kayvon Fatahalian, and Christopher Ré. In this talk I will also connect the Mandoline framework to the broader theme of interactive ML systems and some of my collaborators research in this area.
  bio: Mayee Chen is a second year PhD student in Computer Science at Stanford University, advised by Professor Christopher Ré. She is interested in understanding the theoretical underpinnings of tools in modern machine learning and using them to develop new methods. Her current interests revolve around how to evaluate sources of supervision (e.g., weakly, semi-supervised, and self-supervised) throughout the ML pipeline, particularly through both information-theoretic and geometric lenses. Previously, she completed her undergraduate studies at Princeton University, majoring in Operations Research and Financial Engineering.
  recording: https://www.youtube.com/watch?v=LmiNQio4db0
  
- speaker: Juan Manuel Zambrano Chaves
  institution: Stanford University
  type: previous
  date: 8/12/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Multimodal opportunistic risk assessment for ischemic heart disease
  abstract: Current risk scores for predicting ischemic heart disease (IHD) risk—the leading cause of global mortality—have limited efficacy. While body composition (BC) imaging biomarkers derived from abdominopelvic computed tomography (CT) correlate with IHD risk, they are impractical to measure manually. Here, in a retrospective cohort of 8,197 contrast-enhanced abdominopelvic CT examinations undergoing up to 5 years of follow-up, we developed improved multimodal opportunistic risk assessment models for IHD by automatically extracting BC features from abdominal CT images and integrating these with features from each patient’s electronic medical record (EMR). Our predictive methods match and, in some cases, outperform clinical risk scores currently used in IHD risk assessment. We provide clinical interpretability of our model using a new method of determining tissue-level contributions from CT along with weightings of EMR features contributing to IHD risk. We conclude that such a multimodal approach, which automatically integrates BC biomarkers and EMR data can enhance IHD risk assessment and aid primary prevention efforts for IHD. In this talk, I will also go over other recent publications related to opportunistic imaging, body composition analysis and cardiovascular disease.
  bio: Juan Manuel pursuing is PhD in Biomedical Informatics at Stanford University, advised by Daniel Rubin and Akshay Chaudhari. He is broadly interested in developing informatics tools to aid clinicians in clinical practice. His current research focuses on multimodal data fusion, building models that leverage medical images in addition to other relevant data sources. He was previously awarded a medical degree in addition to a B.S. in biomedical engineering from Universidad de los Andes in Bogotá, Colombia.
  recording: https://www.youtube.com/watch?v=4nYNY91lI7A
  
- speaker: No session this week -- break! 
  type: previous
  date: 8/19/21
  
- speaker: No session this week -- break! 
  type: previous
  date: 8/26/21
  
- speaker: Beliz Gunel
  institution: Stanford University
  type: previous
  date: 9/2/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Self-training vs. Weak Supervision using Untrained Neural Nets for MR Reconstruction
  abstract: Untrained neural networks use CNN architecture itself as an image prior for reconstructing natural images without requiring any supervised training data. This makes them a compelling tool for solving inverse problems such as denoising and MR reconstruction for which they achieve performance that is on-par with some state-of-the-art supervised methods. However, untrained neural networks require tens of minutes to reconstruct a single MR slice at inference time, making them impractical for clinical deployment. We propose using ConvDecoder to generate “weakly-labeled” data from undersampled MR scans at training time. Using few supervised pairs and constructed weakly supervised pairs, we train an unrolled neural network that gives strong reconstruction performance with fast inference time of few seconds. We show that our method considerably improves over supervised and self-training baselines in the limited data regime while mitigating the slow inference bottleneck of untrained neural networks. In this talk, I will also briefly talk about how self-training can be applied, and in fact be complementary to pre-training approaches, in other application domains such as natural language understanding.
  bio: Beliz Gunel is a fourth year PhD student in Electrical Engineering at Stanford University, advised by Professor John Pauly. Her research interests are primarily in representation learning for medical imaging and natural language processing, and building data-efficient machine learning methods that are robust to distribution drifts. She collaborates closely with Professor Akshay Chaudhari and Professor Shreyas Vasanawala, and had research internships at Google AI, Facebook AI, and Microsoft Research. Previously, she completed her undergraduate studies at University of California, Berkeley, majoring in Electrical Engineering and Computer Science.
  recording: https://www.youtube.com/watch?v=TU2KQVSFa-I
  
- speaker: No session this week -- summer break! 
  type: previous
  date: 9/9/21
  
- speaker: No session this week -- summer break! 
  type: previous
  date: 9/16/21
  

- speaker: Jared Dunnmon (TIME CHANGE - 2PM to 3PM PST)
  institution: Visiting Scholar, Stanford University
  type: previous
  date: 9/23/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: The Many Faces of Weak Supervision in Medical Representation Learning - Harnessing Cross-Modality, Enabling Multi-task Learning, and Mitigating Hidden Stratification
  abstract: Weakly supervised machine learning models have shown substantial promise in unlocking the value of vast stores of medical information in support of clinical decisionmaking.  In this talk, we will discuss several avenues by which these approaches can be used in real-world medical imaging applications, and the value that can be provided in each case.  We first demonstrate how cross-modal weak supervision can be used to train models that achieve results statistically similar to those trained using hand labels, but with orders-of-magnitude less labeling effort.  We then build on this idea to show how the large-scale multi-task learning made practical by weak supervision can provide value by supporting anatomically-resoved models for volumetric medical imaging applications.  Finally, we discuss recent results indicating that weakly supervised distributionally robust optimization can be used to improve model robustness in an automated way.
  bio: Dr. Jared Dunnmon is currently a Visiting Scholar in the Department of Biomedical Data Science at Stanford University.  Previously, Jared was an Intelligence Community Postdoctoral Fellow in Computer Science at Stanford, where he was advised by Profs. Chris Ré and Daniel Rubin.  His research interests focus on combining heterogeneous data modalities, machine learning, and human domain expertise to inform and improve decisionmaking around such topics as human health, energy & environment, and geopolitical stability.  Jared has also worked to bridge the gap between technological development and effective deployment in a variety of contexts including foreign policy at the U.S. Senate Foreign Relations Committee, solar electrification at Offgrid Electric, cybersecurity at the Center for Strategic and International Studies, emerging technology investment at Draper Fisher Jurvetson, nuclear fusion modeling at the Oxford Mathematical Institute, and nonlinear energy harvesting at Duke University.  Jared holds a PhD from Stanford University (2017), a B.S. from Duke University, and both an MSc. in Mathematical Modeling and Scientific Computing and an M.B.A. from Oxford, where he studied as a Rhodes Scholar. 
  recording: https://www.youtube.com/watch?v=oeiy0COOuco
  
  
- speaker: Siyi Tang
  institution: Stanford University
  type: previous
  date: 9/30/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Graph-based modeling in computational pathology
  abstract: Advances in whole-slide imaging, deep learning, and computational power have enabled substantial growth in the field of computational pathology, including automating routine pathology workflows and discovery of novel biomarkers. Convolutional neural networks (CNNs) have been the most commonly used network architecture in computational pathology. However, a different line of work that leverages cellular interactions and spatial structures in whole slide images using graph-based modeling methods is emerging. In this journal club, I will lead a discussion on graph-based modeling, particularly graph neural networks, in the field of computational pathology.
  bio: Siyi Tang is a PhD candidate in Electrical Engineering at Stanford University, advised by Prof. Daniel Rubin. Her research aims to leverage the structure in medical data to develop better medical machine learning models and enable novel scientific discovery. She is also interested in enhancing the human interpretability and clinical utility of medical machine learning algorithms. Prior to Stanford, Siyi received her Bachelor of Engineering Degree with Highest Distinction Honor from National University of Singapore.
  recording: https://youtu.be/RHydd3mYowM
  
- speaker: Jonathan Crabbé
  institution: University of Cambridge
  type: previous
  date: 10/7/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Explainable AI - from generalities to time series
  abstract: Modern machine learning models are complicated. They typically involve millions of operations to turn their input into a prediction. Hence, in a human perspective, they are complete black-boxes. When these models are used in critical areas such as medicine, finance and the criminal justice system, this lack of transparency appears as a major hindrance to their adoption. With the necessity to address this problem, the field of Explainable AI (XAI) thrived. In this talk, we will first illustrate how XAI allows to achieve a better understanding of these complex machine learning models in general. We will then focus on model for time series data, which constitutes a big portion of the medical data. 
  bio: Jonathan Crabbé is a PhD student in the Department of Applied Mathematics from the University of Cambridge, he is supervised by Mihaela van der Schaar. He joins the van der Schaar lab following a MASt in in theoretical physics and applied mathematics at Cambridge, which he passed with distinction, receiving the Wolfson College Jennings Price. <br>Jonathan’s work focuses on the development of explainable artificial intelligence (XAI), which he believes to be one of the most interesting challenges in machine learning. He is particularly interested in understanding the structure of the latent representations learned by state of the art models. With his theoretical physics background, Jonathan is also enthusiastic about time series models and forecasting. 
  recording: https://www.youtube.com/watch?v=U292VpcNqgM
  
- speaker: Jean-Benoit Delbrouck
  institution: Stanford University
  type: previous
  date: 10/14/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Multimodal medical research at the intersection of vision and language
  abstract: Inspired by traditional machine learning on natural images and texts, new multimodal medical tasks are emerging. From Medical Visual Question Answering to Radiology Report Generation or Summarization using x-rays, we investigate how multimodal architectures and multimodal pre-training can help improving results.
  bio: Jean-Benoit holds a PhD in engineering science from Polytechnic Mons in Belgium and is now a postdoctoral scholar at the Departement of Biomedical Data Science . His doctoral thesis focused on multimodal learning on natural images and texts. His postdoctoral research focuses on applying new (or proven) methods on multimodal medical tasks at the intersection of vision and language.
  recording:  https://www.youtube.com/watch?v=dv0azbOgTZE
  
- speaker: Khaled Saab
  institution: Stanford University
  type: previous
  date: 10/21/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Observational Supervision for Medical Image Classification using Gaze Data
  abstract: Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during clinical workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). You can find details in our <a href="https://web.stanford.edu/~ksaab/media/MICCAI_2021.pdf">MICCAI 2021 paper</a>.
  bio: Khaled Saab is a PhD student at Stanford, co-advised by Daniel Rubin and Christopher Re. His research interests are in developing more sustainable and reliable ML models for healthcare applications. Khaled is a Stanford Interdisciplinary Graduate Fellow, one of the greatest honors Stanford gives to a doctoral student pursuing interdisciplinary research.
  recording: https://www.youtube.com/watch?v=va99KCqMbcA
  
- speaker: Sarah Hooper
  institution: Stanford University
  type: previous
  date: 10/28/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Training medical image segmentation models with less labeled data
  abstract: Segmentation is a powerful tool for quantitative analysis of medical images. Because manual segmentation can be tedious, be time consuming, and have high inter-observer variability, neural networks (NNs) are an appealing solution for automating the segmentation process. However, most approaches to training segmentation NNs rely on large, labeled training datasets that are costly to curate. In this work, we present a general semi-supervised method for training segmentation networks that reduces the required amount of labeled data. Instead, we rely on a small set of labeled data and a large set of unlabeled data for training. We evaluate our method on four cardiac magnetic resonance (CMR) segmentation targets and show that by using only 100 labeled training image slices---up to a 99.4% reduction of labeled data---the proposed model achieves within 1.10% of the Dice coefficient achieved by a network trained with over 16,000 labeled image slices. We use the segmentations predicted by our method to derive cardiac functional biomarkers and find strong agreement to expert measurements of predicted ejection fraction, end diastolic volume, end systolic volume, stroke volume, or left ventricular mass compared an expert annotator.  
  bio: Sarah Hooper is a PhD candidate at Stanford University, where she works with Christopher Ré and Curtis Langlotz. She is broadly interested in applying machine learning to meet needs in healthcare, with a particular interest in applications that make quality healthcare more accessible. Sarah received her B.S. in Electrical Engineering at Rice University in 2017 and her M.S. in Electrical Engineering at Stanford University in 2020.
  recording: https://www.youtube.com/watch?v=FTmvdARsbnI
  
- speaker: Mars Huang (TIME CHANGE - 9AM to 10AM PST)
  institution: Stanford University
  type: previous
  date: 11/4/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Towards Generalist Medical Imaging AI Using Multimodal Self-supervised Learning
  abstract: In recent years, deep learning models have demonstrated superior diagnostic accuracy compared to human physicians in several medical domains and imaging modalities. While deep learning and computer vision provide promising solutions for automating medical image analysis, annotating medical imaging datasets requires domain expertise and is cost-prohibitive at scale. Therefore, the task of building effective medical imaging models is often hindered by the lack of large-scale manually labeled datasets. In a healthcare system where myriad opportunities and possibilities for automation exist, it is practically impossible to curate labeled datasets for all tasks, modalities, and outcomes for training supervised models. Therefore, it is important to develop strategies for training generalist medical AI models without the need for large-scale labeled datasets. In this talk, I will talk about how our group plan to develop generalist medical imaging models by combining multimodal fusion techniques with self-supervised learning. 
  bio: Mars Huang is a 3rd year Ph.D. student in Biomedical Informatics at Stanford University, co-advised by Matthew P. Lungren and Serena Yeung. He is interested in combining self-supervised learning and multimodal fusion techniques for medical imaging applications. Previously, he completed his undergraduate studies at the University of California, San Diego, majoring in Computer Science and Bioinformatics.
  recording: https://www.youtube.com/watch?v=ToJkbcC1N80 
  
- speaker: Rocky Aikens
  institution: Stanford University
  type: previous
  date: 11/11/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Assignment Control Plots - A Visual Companion for Causal Inference Study Design
  abstract:  An important step for any causal inference study design is understanding the distribution of the treated and untreated subjects in terms of measured baseline covariates.  However, not all baseline variation is equally important.  In the observational context, balancing on baseline variation summarized in a propensity score can help reduce bias due to self-selection. In both observational and experimental studies, controlling baseline variation associated with the expected outcomes can help increase the precision of causal effect estimates. We propose a set of visualizations that decompose the space of measured covariates into the different types of baseline variation important to the study design.  These assignment-control plots and variations thereof visually illustrate core concepts of causal inference and suggest new directions for methodological research on study design. As a practical demonstration, we illustrate one application of assignment-control plots to a study of cardiothoracic surgery. While the family of visualization tools for studies of causality is relatively sparse, simple visual tools can be an asset to education, application, and methods development.  (This work is in the peer-review process and is currently available as a preprint on arxiv https://arxiv.org/abs/2107.00122)
  bio: Rachael C. “Rocky” Aikens is a collaborative biostatistician. Her methodological research focuses on the development of simple, data-centered tools and frameworks to design stronger observational studies.  As a collaborator, she has led the statistical analysis of randomized assessments of clinical decision support, clinical informatics applications in pediatrics, and lifestyle interventions for reducing sedentary behavior and improving nutrition.  A central focus of her work is the design and deployment of simple, data-centered methodologies, grounded in the needs of applied researchers.  She is finishing a doctoral degree in Biomedical Informatics at Stanford University with expected completion June 2022. 
  recording: https://www.youtube.com/watch?v=-Vvquyw4fz4
  
- speaker: Ramon Correa
  institution: Emory University
  type: previous
  date: 11/18/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Adversarial debiasing with partial learning - medical image case-studies
  abstract: The use of artificial intelligence (AI) in healthcare has become a very active research area in the last few years. While significant progress has been made in image classification tasks, only a few AI methods are actually being deployed in hospitals. A major hurdle in actively using clinical AI models currently is the trustworthiness of these models. When scrutinized, these models reveal implicit biases during the decision making, such as detecting race, ethnic groups, and subpopulations. These biases result in poor model performance, or racial disparity, for patients in these minority groups. In our ongoing study, we develop a two-step adversarial debiasing approach with partial learning that can reduce the racial disparity while preserving the performance of the targeted task. The proposed methodology has been evaluated on two independent medical image case-studies - chest X-ray and mammograms, and showed promises in reducing racial disparity while preserving the performance.
  bio: Ramon Correa is a Ph.D. student in ASU’s Data Science, Analytics, and Engineering program. His research interest involves studying model debiasing techniques. Previously, he completed his undergraduate studies at Case Western Reserve University, majoring in Biomedical Engineering.
  recording: https://www.youtube.com/watch?v=6AMD5pF2fNE

  
- speaker: No session this week -- Thanksgiving break
  institution: Stanford University
  type: previous
  date: 11/25/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
 
  
- speaker: Winter Break -- we will see you next year :)
  type: previous
  date: 12/2/21 to 1/13/22
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  

- speaker: Lequan Yu (TIME CHANGE - 4PM to 5PM PST)
  institution: University of Hong Kong
  type: previous
  date: 1/20/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Medical Image Analysis and Reconstruction with Data-efficient Learning
  abstract: Medical imaging is a critical step in modern healthcare procedures. Accurate interpretation of medical images, e.g., CT, MRI, Ultrasound, and histology images, plays an essential role in computer-aided diagnosis, assessment, and therapy. While deep learning provides an avenue to deliver automated medical image analysis and reconstruction via data-driven representation learning, the success is largely attributed to the massive datasets with abundant annotations. However, collecting and labeling such large-scaled dataset is prohibitively expensive and time-consuming. In this talk, I will present our recent works on building data-efficient learning systems for medical image analysis and reconstruction, such as computer-aided diagnosis, anatomical structure segmentation, and CT reconstruction. The proposed methods cover a wide range of deep learning and machine learning topics, including semi-supervised learning, multi-modality learning, multi-task learning, integrating domain knowledge, etc. The up-to-date progress and promising future directions will also be discussed. 
  bio: Dr. Lequan Yu is an Assistant Professor at the Department of Statistics and Actuarial Science, the University of Hong Kong. Before joining HKU, he was a postdoctoral fellow at Stanford University. He obtained his Ph.D. degree from The Chinese University of Hong Kong in 2019 and Bachelor’s degree from Zhejiang University in 2015, both in Computer Science. He also experienced research internships in Nvidia and Siemens Healthineers. His research interests are developing advanced machine learning methods for biomedical data analysis, with a primary focus on medical images. He has won the CUHK Young Scholars Thesis Award 2019, Hong Kong Institute of Science Young Scientist Award shortlist in 2019, Best Paper Awards of Medical Image Analysis-MICCAI in 2017 and International Workshop on Machine Learning in Medical Imaging in 2017. He serves as the senior PC member of IJCAI, AAAI, and the reviewer for top-tier journals and conferences, such as Nature Machine Intelligence, IEEE-PAMI, IEEE-TMI, Medical Image Analysis, etc. 
  recording: https://www.youtube.com/watch?v=HhqgbVnR2ZQ
  
- speaker: Jason Jeong
  institution: Arizona State University
  type: previous
  date: 1/27/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Applications of Generative Adversarial Networks (GANs) in Medical Image Synthesis, Translation, and Augmentation
  abstract: Medical imaging is a source of crucial information in modern healthcare. Deep learning models have been developed for various modalities such as CT, MRI, Ultrasound, and PET for automatic or semi-automatic diagnosis or assessment of diseases. While deep learning models have been proven to be very powerful, training such models sufficiently requires large, well-annotated but expensive datasets. However, medical images, especially those containing diseases, are rare. While there are a variety of solutions to improve models with limited and imbalanced datasets, one solution is generating these rare images through generative adversarial networks (GANs). In this presentation, I will present a quick review on the use of GANs in medical imaging tasks, specifically classification and segmentation. Then I will present and discuss our recent work on using GANs for generating synthetic dual energy CT (sDECT) from single energy CT (SECT). Finally, some interesting challenges and possible future directions of GANs in medical imaging will be discussed.
  bio: Jiwoong Jason Jeong is a Ph.D. student in ASU’s Data Science, Analytics, and Engineering program. His research interest involves applying GANs into the medical workflow with a focus on solving medical data imbalance and scarcity. Previously, he completed his Master’s in Medical Physics at Georgia Institute of Technology.  
  recording: https://www.youtube.com/watch?v=cz3_ckbdiKw
  
- speaker: Jeffrey Gu
  institution: Stanford University
  type: previous
  date: 2/3/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Towards Unsupervised Biomedical Image Segmentation using Hyperbolic Representations
  abstract: Segmentation models are extremely useful for biomedical image analysis, but training segmentation models often require large, labelled datasets that are difficult and costly to acquire. Unsupervised learning is a promising approach for training segmentation models that avoids the need to acquire labelled datasets, but is made difficult by the lack of high-quality supervisory signal from expert annotations. Using the observation that biomedical images often contain an inherent hierarchical structure, we augment a VAE with additional supervisory signal via a novel self-supervised hierarchical loss. To aid the learning of hierarchical structure, we learn hyperbolic representations instead of Euclidean representations. Hyperbolic representations have previously been employed in fields such as natural language processing (NLP) as a way to learn hierarchical and tree-like structures, making them a natural choice of representation. In this talk, I will discuss hyperbolic representations for biomedical imaging as well as our recent paper on the topic. 
  bio: Jeffrey Gu is a 2nd year Ph.D. student in ICME at Stanford University advised by Serena Yeung. His research interests include representation learning, unsupervised learning, biomedical applications, and beyond. Prior to Stanford, Jeffrey completed his undergraduate studies at the California Institute of Technology, majoring in Mathematics. 
  recording: https://www.youtube.com/watch?v=UabE4YR3uU8
  
- speaker: Enze Xie (TIME CHANGE - 4PM to 5PM PST)
  institution: University of Hong Kong
  type: previous
  date: 2/10/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: SegFormer - Simple and Efficient Design for Semantic Segmentation with Transformers
  abstract: We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features- 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code is available <a href="https://github.com/NVlabs/SegFormer">here</a>.
  bio: <a href="https://xieenze.github.io/" target="_blank">Enze Xie</a> is currently a PhD student in the Department of Computer Science, The University of Hong Kong. His research interest is computer vision in 2D and 3D. He has published 16 papers (including 10 first/co-first author) in top-tier conferences and journals such as TPAMI, NeurIPS, ICML and CVPR with 1400+ citations.  His work PolarMask was selected as CVPR 2020 Top-10 Influential Papers. He was selected into NVIDIA Graduate Fellowship Finalist. He has won 1st place in Google OpenImages 2019 instance segmentation track.
  recording: https://www.youtube.com/watch?v=Yf9fNn1fWy8
  
- speaker: Weston Hughes
  institution: Stanford University
  type: previous
  date: 2/17/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Deep Learning Methods for Electrocardiograms and Echocardiograms
  abstract: In this talk, we will discuss two recently published deep learning methods we’ve developed at Stanford and UCSF for the understanding of ECG and echocardiogram data. First, we'll discuss the development and evaluation of a convolutional neural network for multi class ECG interpretation which outperforms cardiologists and currently used ECG algorithms. Second, we’ll discuss a computer vision system for evaluating a range of biomarkers from echocardiogram videos. In our discussion of both papers, we’ll emphasize different analyses aiming to explain and interpret the models in different ways.
  bio: Weston Hughes is a 3rd year PhD student in the Computer Science department at Stanford, co-advised by James Zou in Biomedical Data Science and Euan Ashley in Cardiology. His research focuses on applying deep learning and computer vision techniques to cardiovascular imaging data, including electrocardiograms, echocardiograms and cardiac MRIs. He is an NSF Graduate Research Fellow. 
  recording: https://www.youtube.com/watch?v=ZQu3zNk7_4Y
  
- speaker: Mike Wu
  institution: Stanford University
  type: previous
  date: 2/24/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Optimizing for Interpretability in Deep Neural Networks
  abstract: Deep models have advanced prediction in many domains, but their lack of interpretability remains a key barrier to their adoption in many real world applications. There exists a large body of work aiming to help humans understand these black box functions to varying levels of granularity – for example, through distillation, gradients, or adversarial examples. These methods however, all tackle interpretability as a separate process after training. In this talk, we explore a different approach and explicitly regularize deep models so that they are well-approximated by processes that humans can step through in little time. Applications will focus on medical prediction tasks for patients in critical care and with HIV.
  bio: Mike is a fifth year PhD student in Computer Science at Stanford University advised by Prof. Noah Goodman. His primary research interests are in deep generative models and unsupervised learning algorithms, often with applications to education and healthcare data. Mike’s research has been awarded two best paper awards at AAAI and Education Data Mining as well as featured in the New York Times. Prior to Stanford, Mike was a research engineer at Facebook’s applied ML group.
  recording: https://www.youtube.com/watch?v=c4bl3E6MdhE

- speaker: Siyi Tang
  institution: Stanford University
  type: previous
  date: 3/3/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis
  abstract: Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. In this talk, I will present our recent work on graph-based modeling for EEG-based seizure detection and classification. We model EEG signals using a graph neural network and develop two EEG graph structures that capture the natural geometry of EEG sensors or dynamic brain connectivity. We also propose a self-supervised pre-training strategy to further improve the model performance, particularly on rare seizure types. Lastly, we investigate model interpretability and propose quantitative metrics to measure the model’s ability to localize seizures. <a href="https://openreview.net/pdf?id=k9bx1EfHI_-">ICLR paper link</a>
  bio: Siyi Tang is a PhD candidate in Electrical Engineering at Stanford University, advised by Prof. Daniel Rubin. Her research aims to leverage the structure in medical data to develop better medical machine learning models and enable novel scientific discovery. She is also interested in enhancing the human interpretability and clinical utility of medical machine learning algorithms. Prior to Stanford, Siyi received her Bachelor of Engineering Degree with Highest Distinction Honor from National University of Singapore.
  recording: https://www.youtube.com/watch?v=FMEnSYbKkV4

- speaker: Bin Li
  institution: University of Wisconsin-Madison
  type: previous
  date: 3/10/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Weakly supervised tumor detection in whole slide image analysis
  abstract: Histopathology is one of the essential tools for disease assessment. In modern histopathology, whole slide imaging (WSI) has become a powerful and widely used tool to visualize tissue sections in disease diagnosis, medical education, and pathological research. The use of machine learning brings great opportunities to the automatic analysis of WSIs that could facilitate the pathologists’ workflow and more importantly, enable higher-order or large-scale correlations that are normally very challenging in standard histopathology practices, such as differential diagnosis of hard-cases and treatment response predictions. This talk will cover our recent work of approaching the fundamental problem of weakly supervised classification and tumor localization in gigapixel WSIs with a novel multiple instance learning (MIL) model leveraged by self-supervised learning, as well as discussing the emerging challenges and opportunities in computational histopathology.
  bio: Bin Li is a Ph.D. candidate in Biomedical Engineering at the University of Wisconsin-Madison. He is currently a research assistant in the Laboratory for Optical and Computational Instrumentation, mentored by Prof. Kevin Eliceiri. He develops computational methods to improve the understanding of the biological and pathological mechanisms of disease development and patient care based on multi-modal microscopic image analysis.
  recording: https://www.youtube.com/watch?v=ZPe94q8wxPQ

  
- speaker: Mikhail Khodak
  institution: Carnegie Mellon University
  type: previous
  date: 3/17/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Federated Hyperparameter Tuning - Challenges, Baselines, and Connections to Weight-Sharing
  abstract: Tuning hyperparameters is a crucial but arduous part of the machine learning pipeline. Hyperparameter optimization is even more challenging in federated learning, where models are learned over a distributed network of heterogeneous devices; here, the need to keep data on device and perform local training makes it difficult to efficiently train and evaluate configurations. In this work, we investigate the problem of federated hyperparameter tuning. We first identify key challenges and show how standard approaches may be adapted to form baselines for the federated setting. Then, by making a novel connection to the neural architecture search technique of weight-sharing, we introduce a new method, FedEx, to accelerate federated hyperparameter tuning that is applicable to widely-used federated optimization methods such as FedAvg and recent variants. Theoretically, we show that a FedEx variant correctly tunes the on-device learning rate in the setting of online convex optimization across devices. Empirically, we show that FedEx can outperform natural baselines for federated hyperparameter tuning by several percentage points on the Shakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using the same training budget.
  bio: Misha is a PhD student in computer science at Carnegie Mellon University advised by Nina Balcan and Ameet Talwalkar. His research focuses on foundations and applications of machine learning, in particular the theoretical and practical understanding of meta-learning and automation. He is a recipient of the Facebook PhD Fellowship and has spent time as an intern at Microsoft Research - New England, the Lawrence Livermore National Lab, and the Princeton Plasma Physics Lab. Previously, he received an AB in Mathematics and an MSE in Computer Science from Princeton University.
  recording: https://www.youtube.com/watch?v=f7tG0XHwtuQ

  
- speaker: Karan Singhal
  institution: Google Research
  type: previous
  date: 3/24/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Generalization and Personalization in Federated Learning with Connections to Medical AI
  abstract: Karan will present two recent works - "What Do We Mean by Generalization in Federated Learning?" (to appear at ICLR 2022, <a href="https://arxiv.org/abs/2110.14216">paper</a>) and "Federated Reconstruction- Partially Local Federated Learning" (presented at NeurIPS 2021, <a href="https://arxiv.org/abs/2102.03448">paper</a>, <a href="https://ai.googleblog.com/2021/12/a-scalable-approach-for-partially-local.html">blog post</a>). He'll give an overview of federated learning, discuss how we might think about generalization when we have multiple local data distributions, and provide an example of a method that improves final generalization to new data distributions. Throughout the talk, he'll connect the works to medical AI by discussing generalization to unseen patients and hospitals, in both federated and standard centralized settings.
  bio: Karan leads a team of engineers and researchers at Google Research working on representation learning and federated learning, with applications in medical AI. He is broadly interested in developing and validating techniques that lead to wider adoption of AI that benefits people. Prior to joining Google, he received an MS and BS in Computer Science from Stanford University.
  recording: https://youtu.be/jthG7xHvGmo

- speaker: Max Lu
  institution: MIT & Harvard Medical School
  type: previous
  date: 3/31/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Weakly-supervised, large-scale computational pathology for diagnosis and prognosis
  abstract: In this talk, I will outline a general framework for developing interpretable diagnostic and prognostic machine learning models based on digitized histopathology slides. Our method does not require manual annotation of regions of interest and can be easily scaled to tens of thousands of samples. Examples of application range from cancer subtyping and prognosis to predicting the primary origins of metastatic tumors. 
  bio: Max is a 1st year Computer Science PhD student at MIT advised by Dr. Faisal Mahmood, currently interested in computational pathology and spatial biology. He obtained his B.S. degree in biomedical engineering and applied math and statistics from Johns Hopkins University. Before starting his PhD, his research primarily focused on developing machine learning algorithms for large scale quantitative analysis of digital histopathology slides.
  recording: https://www.youtube.com/watch?v=vOptYqkxgwg
  
- speaker: No session this week -- Spring Break! 
  type: previous
  date: 4/7/2022
  
- speaker: Sabri Eyuboglu
  institution: Stanford University
  type: previous
  date: 4/14/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Discovering Systematic Errors with Domino
  abstract: Machine learning models that achieve high overall accuracy often make systematic errors on coherent slices of validation data. In this talk , I introduce Domino, a new approach for discovering these underperforming slices. I also discuss a new framework for quantitatively evaluating methods like Domino.
  bio: Sabri is a 2nd Year CS PhD Student in the Stanford Machine Learning Group co-advised by Chris Ré and James Zou. He’s broadly interested in methods that make machine learning systems more reliable in challenging applied settings. To that end, he’s recently been working on tools that help practitioners better understand the interaction between their models and their data.
  recording:  https://www.youtube.com/watch?v=4sN_du_U05A

- speaker: Albert Gu
  institution: Stanford University
  type: previous
  date: 4/21/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Efficiently Modeling Long Sequences with Structured State Spaces
  abstract: A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies.  Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps.  This talk introduces the Structured State Space sequence model (S4), a simple new model based on the fundamental state space representation $x*(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t)$. S4 combines elegant properties of state space models with the recent HiPPO theory of continuous-time memorization, resulting in a class of structured models that handles long-range dependencies mathematically and can be computed very efficiently.  S4 achieves strong empirical results across a diverse range of established benchmarks, particularly for continuous signal data such as images, audio, and time series.
  bio: Albert Gu is a final year Ph.D. candidate in the Department of Computer Science at Stanford University, advised by Christopher Ré. His research broadly studies structured representations for advancing the capabilities of machine learning and deep learning models, with focuses on structured linear algebra, non-Euclidean representations, and theory of sequence models. Previously, he completed a B.S. in Mathematics and Computer Science at Carnegie Mellon University, and an internship at DeepMind in 2019.
  recording: https://www.youtube.com/watch?v=luCBXCErkCs
  
  
- speaker: Petar Stojanov
  institution: Broad Institute
  type: previous
  date: 4/28/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Domain Adaptation with Invariant Representation Learning - What Transformations to Learn?
  abstract: Unsupervised domain adaptation, as a prevalent transfer learning setting, spans many real-world applications. With the increasing representational power and applicability of neural networks, state-of-the-art domain adaptation methods make use of deep architectures to map the input features X to a latent representation Z that has the same marginal distribution across domains. This has been shown to be insufficient for generating optimal representation for classification, and to find conditionally invariant representations, usually strong assumptions are needed. We provide reasoning why when the supports of the source and target data from overlap, any map of X that is fixed across domains may not be suitable for domain adaptation via invariant features. Furthermore, we develop an efficient technique in which the optimal map from X to Z also takes domain-specific information as input, in addition to the features X. By using the property of minimal changes of causal mechanisms across domains, our model also takes into account the domain-specific information to ensure that the latent representation Z does not discard valuable information about Y . We demonstrate the efficacy of our method via synthetic and real-world data experiments. The code is available at https://github.com/DMIRLAB-Group/DSAN.
  bio: Petar (<a href="http://petar-stojanov.com/">website</a>) is a postdoctoral researcher at the Broad Institute of MIT and Harvard, where he is supervised by Prof. Gad Getz and Prof. Caroline Uhler. He received his PhD in Computer Science at Carnegie Mellon University, where he was fortunate to be advised by Prof. Jaime Carbonell and Prof. Kun Zhang. Prior to that, he was an associate computational biologist at the Getz Lab. His research interests span machine learning and computational biology. He is currently very interested in applying causal discovery methodology to improve genomic analysis of cancer mutation and single-cell RNA sequencing data with the goal of understanding relevant causal relationships in cancer progression. His doctoral research was in transfer learning and domain adaptation from the causal perspective, a field which he is still interested and active.
  recording: https://www.youtube.com/watch?v=5A-OnX6pGus
  

- speaker: Nandita Bhaskhar
  institution: Stanford University
  type: previous
  date: 5/5/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Beyond Test Set Performance - Rethinking Generalization Strategies for Clinical Deployment
  abstract: Artificial Intelligence (AI) and Deep Learning (DL) have seen tremendous successes across various domains in medicine. However, most of these successes have been limited to academic research, with their performance validated on siloed datasets. Real world deployment of deep learning models in clinical practice are rare. In this talk, I will discuss several studies and papers in a journal club format that demonstrate various challenges and risks in directly deploying current day models to the clinic. I will then lead a discussion surrounding strategies and recommendations for developing an evaluation framework and monitoring system to make our models suitable for deployment.
  bio: Nandita Bhaskhar (see <a href="http://web.stanford.edu/~nanbhas/" target="_blank">website</a>) is a PhD student in the Department of Electrical Engineering at Stanford University advised by Daniel Rubin. She is broadly interested in developing machine learning methodology for medical applications. Her current research focuses on (i) building label-efficient models through observational supervision and self-supervision for leveraging unlabelled medical data and (ii) developing strategies for reliable model deployment by assessing, quantifying and enhancing model trust, robustness to distribution shifts, etc. Prior to Stanford, she received her B.Tech in Electronics Engineering from the Indian Institute of Information Technology, IIIT, with the highest honours. 
  recording: https://youtu.be/EKEAe0KEwkw
  
- speaker: Ramon Correa
  institution: Arizona State University
  type: previous
  date: 5/12/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title:  A review of <em>Fair</em> AI model development for image classification and prediction
  abstract: Artificial Intelligence (AI) models have demonstrated expert-level performance in image-based recognition and diagnostic tasks, resulting in increased adoption and FDA approvals for clinical applications. The new challenge in AI is to understand the limitations of models to reduce potential harm. Particularly, unknown disparities based on demographic factors could encrypt currently existing inequalities worsening patient care for some groups. In this talk, we will discuss techniques to improve model fairness for medical imaging applications alongside their limitations. 
  bio: Ramon Correa is a Ph.D. student in ASU’s Data Science, Analytics, and Engineering program. His research interest involves studying model debiasing techniques. Previously, he completed his undergraduate studies at Case Western Reserve University, majoring in Biomedical Engineering.
  recording: https://www.youtube.com/watch?v=ZyW57DBJxT8
  
- speaker: Xiaoyuan Guo
  institution: Emory Univeristy
  type: previous
  date: 5/19/2022
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Facilitating the Curation and Future Analysis of Annotated Medical Images Across Institutions
  abstract: Medical imaging plays a significant role in different clinical applications such as detection, monitoring, diagnosis, and treatment evaluations of various clinical conditions. Supervised deep learning approaches have been popular in solving medical image related tasks. However, training such models often require large amounts of annotated data as supervision, which is often unavailable in the medical area. Therefore, curation of annotated data is promising to create a large-scale dataset and contribute to the development of supervised learning. Nonetheless, directly sharing data is prohibited due to the patient concerns. Without exchanging data between internal and external data sources, we propose to apply unsupervised anomaly detectors on the internal dataset and learn the clean in-distribution (ID). Then we share the trained models with the externals and detect the class-wise shift data (aka. out-of-distribution (OOD) data) with the anomaly detectors. Higher anomaly scores indicate more difference the external data owe. We also suggest the quantification methods to measure the shiftness of detected data and the external dataset quality after removing the shift samples. Furthermore, we design a corresponding content-based medical image retrieval method that can balance both the intra- and inter-class variance for OOD-sensitive retrieval. The designed shift data identification pipeline can be used to help detect noisy and under-represented data automatically, accelerating the curation process. Meanwhile, the OOD-aware image retrieval is suitable for image annotation, querying and future analysis in external datasets. 
  bio: Xiaoyuan Guo is a Computer Science PhD student at Emory University, working with Prof. Imon Banerjee, Prof. Hari Trivedi and Prof. Judy Wawira Gichoya. Her primary research interests are computer vision and medical image processing. She is experienced in medical image segmentation, out-of-distribution detection, image retrieval, unsupervised learning, etc.
  recording: https://www.youtube.com/watch?v=ZZMLIxOy_Lc
  
- speaker: Laura Manduchi
  institution: ETH Zurich
  type: previous
  date: 5/26/2022
  title: Incorporating domain knowledge in deep generative models for weakly supervised clustering with applications to survival data
  abstract: The ever-growing amount of data and the time cost associated with its labeling have made clustering a relevant task in machine learning. Yet, in many cases, a fully unsupervised clustering algorithm might naturally find a solution that is not consistent with the domain knowledge. Additionally, practitioners often have access to prior information about the types of clusters that are sought, and a principled method to guide the algorithm towards a desirable configuration is then needed. This talk will explore how to integrate domain knowledge, in the form of pairwise constraints and survival data, in deep generative models. Leveraging side information in biomedical datasets enables exploratory analysis of complex data types, resulting in medically meaningful findings.
  bio: Laura is a PhD student in Computer Science at the Institute of Machine Learning at ETH Zürich under the supervision of Julia Vogt and Gunnar Rätsch. She is a member of the Medical Data Science group and of the ETH AI Centre. Her research lies at the interplay between probabilistic modelling and deep learning, with a focus on representation learning, deep generative models, and clustering algorithms. She is particularly interested in incorporating domain knowledge in the form of constraints and probabilistic relations to obtain preferred representations of data that are robust to biases, with applications in medical imaging and X-ray astronomy.
  recording: https://www.youtube.com/watch?v=6gxRghUCXZU
  
- speaker: Huaxiu Yao
  institution: Stanford University
  type: previous
  date: 6/2/2022
  title: Actionable Machine Learning for Tackling Distribution Shift
  abstract: To deploy machine learning algorithms in real-world applications, we must pay attention to distribution shift. When the test distribution differs from the training distribution, there will be a substantial degradation in model performance. To tackle the distribution shift, in this talk, I will present two paradigms with some instantiations. Concretely, I will first discuss how to build machine learning models that are robust to two kinds of distribution shifts, including subpopulation shift and domain shift. I will then discuss how to effectively adapt the trained model to the test distribution with minimal labeled data. The remaining challenges and promising future research directions will also be discussed.
  bio: Huaxiu Yao is a Postdoctoral Scholar in Computer Science at Stanford University, working with Prof. Chelsea Finn. Currently, his research focuses on building machine learning models that are robust to distribution shifts. He is also passionate about applying these methods to solve real-world problems with limited data. He obtained his Ph.D. degree from Pennsylvania State University. The results of his work have been published in top-tier venues such as ICML, ICLR, NeurIPS. He organized the MetaLearn workshop at NeurIPS, the pre-training workshop at ICML, and he served as a tutorial speaker at KDD, IJCAI, and AAAI.
  recording: https://www.youtube.com/watch?v=iVk-ouzXiXA
  
- speaker: SUMMER BREAK!
  type: previous
  date: 6/9/2022
  
- speaker: SUMMER BREAK!
  type: previous
  date: 6/16/2022
  
  
- speaker: Dylan Slack
  institution: University of California, Irvine
  type: previous
  date: 6/23/2022
  title: Exposing Shortcomings and Improving the Reliability of Machine Learning Explanations
  abstract: For domain experts to adopt machine learning (ML) models in high-stakes settings such as health care and law, they must understand and trust model predictions. As a result, researchers have proposed numerous ways to explain the predictions of complex ML models. However, these approaches suffer from several critical drawbacks, such as vulnerability to adversarial attacks, instability, inconsistency, and lack of guidance about accuracy and correctness. For practitioners to safely use explanations in the real world, it is vital to properly characterize the limitations of current techniques and develop improved explainability methods. This talk will describe the shortcomings of explanations and introduce current research demonstrating how they are vulnerable to adversarial attacks. I will also discuss promising solutions and present recent work on explanations that leverage uncertainty estimates to overcome several critical explanation shortcomings.
  bio: Dylan Slack is a Ph.D. candidate at UC Irvine advised by Sameer Singh and Hima Lakkaraju and associated with UCI NLP, CREATE, and the HPI Research Center. His research focuses on developing techniques that help researchers and practitioners build more robust, reliable, and trustworthy machine learning models. In the past, he has held research internships at GoogleAI and Amazon AWS and was previously an undergraduate at Haverford College advised by Sorelle Friedler where he researched fairness in machine learning.
  recording: https://www.youtube.com/watch?v=Eta5Z_ZXrMY
  
  
- speaker: Shibani Santurkar
  institution: Stanford University
  type: previous
  date: 6/30/2022
  title: Actionably interpretable ML
  abstract: Machine learning models today attain impressive accuracy on benchmark tasks. But as we move towards deploying these models in the real world, it becomes increasingly important to verify that they not only make the *right prediction*, but that they do so for the *right reasons*. The scale and complexity of current models however presents a major roadblock in achieving this goal.<br> In this talk, I will discuss a methodology to design neural networks that are accurate, yet at the same time inherently more debuggable. As we demonstrate via numerical and human experiments, our approach yields vision and language models wherein one can more easily pinpoint learned spurious correlations, explain misclassifications, and diagnose biases.
  bio: Shibani Santurkar is a postdoctoral researcher at Stanford University with Tatsu Hashimoto, Percy Liang and Tengyu Ma. Her research revolves around developing machine learning models that can perform reliably in the real world, and characterizing the consequences if they fail to do so. Shibani received a PhD in Computer Science from MIT in 2021, where she was advised by Aleksander Mądry and Nir Shavit. Prior to that, she obtained a B.Tech and M.Tech in electrical engineering from the Indian Institute of Technology Bombay. She is a recipient of the Google Fellowship and an Open Philanthropy early-career grant.
  
- speaker: Ruishan Liu
  institution: Stanford University
  type: previous
  date: 7/7/2022
  title: AI for Clinical Trials and Precision Medicine
  abstract: Clinical trials are the gate-keeper of medicine but can be very costly and lengthy to conduct. Precision medicine transforms healthcare but is limited by available clinical knowledge. This talk explores how AI can help both — make clinical trials more efficient and generate hypotheses for precision medicine. I will first discuss Trial Pathfinder, a computational framework that simulates synthetic patient cohorts from medical records to optimize cancer trial designs (Liu et al. Nature 2021). Trial Pathfinder enables inclusive criteria and data valuation for clinical trials, benefiting diverse patients and trial sponsors. In the second part, I will discuss how to quantify the effectiveness of cancer therapies in patients with specific mutations (Liu et al. Nature Medicine 2022). This work demonstrates how computational analysis of large real-world data generates insights, hypotheses and resources to enable precision oncology.
  bio: Ruishan Liu is a postdoctoral researcher in the Department of Biomedical Data Science at Stanford University, working with Prof. James Zou. She received her PhD in the Department of Electrical Engineering at Stanford University in 2022. She is broadly interested in the intersection of machine learning and applications in human diseases, health and genomics. Her work on Trial Pathfinder was selected as 2021 Top Ten Clinical Research Achievement and Finalist for Global Pharma Award 2021.
  recording: https://www.youtube.com/watch?v=9HVKmTgZJIQ
  
- speaker: Andrew Ilyas
  institution: MIT
  type: previous
  date: 7/14/2022
  title: Datamodels - Predicting Predictions from Training Data
  abstract: Machine learning models tend to rely on an abundance of training data. Yet, understanding the underlying structure of this data--and models exact dependence on it--remains a challenge. In this talk, we will present a framework for directly modeling predictions as functions of training data. This framework, given a dataset and a learning algorithm, pinpoints--at varying levels of granularity--the relationships between train and test point pairs through the lens of the corresponding model class. Even in its most basic version, our framework enables many applications, including discovering data subpopulations, quantifying model brittleness via counterfactuals, and identifying train-test leakage. Based on joint work with Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry.
  bio: Andrew Ilyas is a fourth-year PhD student at MIT, advised by Aleksander Madry and Constantinos Daskalakis. His research focuses on robust and reliable machine learning, with an emphasis on the ways in which (often unintended) correlations present in training data can manifest at test-time. He is supported by an Open Philanthropy Project AI Fellowship. 
  recording: https://www.youtube.com/watch?v=dgo7nRQcw_E
  
- speaker: Hyewon Jeong
  institution: MIT
  type: previous
  date: 7/21/2022
  title: Real-Time Seizure Detection using EEG - A Comprehensive Comparison of Recent Approaches under a Realistic Setting
  abstract: Electroencephalogram (EEG) is an important diagnostic test that physicians use to record brain activity and detect seizures by monitoring the signals. There have been several attempts to detect seizures and abnormalities in EEG signals with modern deep learning models to reduce the clinical burden. However, they cannot be fairly compared against each other as they were tested in distinct experimental settings. Also, some of them are not trained in real-time seizure detection tasks, making it hard for on-device applications. In this work, for the first time, we extensively compare multiple state-of-the-art models and signal feature extractors in a real-time seizure detection framework suitable for real-world application, using various evaluation metrics including a new one we propose to evaluate more practical aspects of seizure detection models.
  bio: Hyewon Jeong, M.D., M.S. is a Ph.D. student in Electrical Engineering and Computer Science at MIT, co-advised by Marzyeh Ghassemi and Collin Stultz. Her primary research focus has been on developing and applying machine learning methods to solve real-world clinical tasks using time-series electronic health record data and signal data. Before joining MIT, she received a B.S. in Biological Sciences, M.S. in Computer Science from KAIST, and M.D. at Yonsei University.
  recording: https://www.youtube.com/watch?v=O-DsfV1_I54
  
- speaker: Richard Chen
  institution: Harvard University
  type: previous
  date: 7/28/2022
  title: Large Images as Long Documents - Rethinking Representation Learning in Gigapixel Pathology Images using Transformers
  abstract: Tissue phenotyping is a fundamental problem in computational pathology (CPATH) that aims at characterizing objective, histopathologic features within gigapixel whole slide images (WSIs) for cancer diagnosis, prognosis, and the estimation of response-to-treatment in patients. However, unlike natural images, whole-slide imaging is a challenging computer vision domain as images can be as large as 150K x 150K pixels, which may be intractable for tasks such as survival prediction which entail modeling complex interactions between morphological visual concepts within the tumor microenvironment. In this talk, we present recent advances that rethink whole-slide images as long documents, adapting Transformer attention for- 1) early-based multimodal fusion with other modalities such as genomics, 2) learning hierarchical representations that capture cells, tissue patterns, and their spatial organization in the tumor microenvironment. In equipping conventional set-based deep learning frameworks in computational pathology with Transformer attention, we present- 1) new improvements on a variety of baselines on slide-level cancer subtyping and survival prediction, 2) new insights on how to perform self-supervision on high-resolution images, and 3) new tasks that shift the evaluation of slide-level tasks in CPATH from the conventional weakly-supervised regime (pixel annotations not needed) to an unsupervised regime (slide annotations not needed).
  bio: Richard Chen is a 4th year Ph.D. student at Harvard University with research interests in multimodal learning, representation learning, and their applications in solving challenging problems in biology and medicine. Previously, he obtained his B.S. / M.S. in Biomedical Engineering and Computer Science at Johns Hopkins University, and also worked as a Researcher at Apple integrating multimodal sensor streams from the iPhone and Apple Watch to measure cognitive decline. In his Ph.D., Richard is currently working on novel computer vision techniques for processing gigapixel images in computational pathology, in particular, 1) representation learning of the the tumor microenvironment,  2) integrative and interpretable techniques for discovering feature correspondences between histology and genomics, and 3) rethinking deep learning approaches for WSIs via advances in NLP - with the analogy that tissue patches in a gigapixel pathology image are words in a document.
  
- speaker: Tri Dao
  institution: Stanford University
  type: previous
  date: 8/4/2022
  title: FlashAttention - Fast and Memory-Efficient Exact Attention with IO-Awareness
  abstract: Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines- 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3× speedup on GPT-2 (seq. length 1K), and 2.4× speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities- the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy). This work received the Best Paper Award at the Hardware-Aware Efficient Training Workshop at ICML, 2022. <a href="https://arxiv.org/abs/2205.14135">Paper</a>, <a href="https://github.com/HazyResearch/flash-attention">Code</a>.
  bio: Tri Dao is a PhD student in Computer Science at Stanford, co-advised by Christopher Re and Stefano Ermon. He works at the interface of machine learning and systems, and his research interests include sequence models with long-range memory and structured matrices for compact deep learning models. His work has received the ICML 2022 Outstanding paper runner-up award.
  recording: https://www.youtube.com/watch?v=FThvfkXWqtE
  
- speaker: No session (Speaker cancellation)
  type: previous
  date: 8/11/2022
  
- speaker: Break - No session
  type: previous
  date: 8/18/2022
  
- speaker: Chima Okechukwu
  institution: Georgia Institute of Technology
  type: previous
  date: 8/25/2022
  title: Lessons learned from an X-ray Body Part Classifier Competition
  abstract: We discuss technical and nontechnical learnings from participating in the UNIFESP X-ray Body Part Classifier Competition. Anyone already in or looking to enter a machine learning competition would benefit from listening to this session.
  bio: Chima Okechukwu is a Masters student in the College of Computing at Georgia Institute of Technology. He is a student under Judy Gichoya in the Healthcare Innovation and Translational Lab (HITI) at Emory. He is broadly interested in machine learning and applications in medical imaging and disease classification.
  recording: https://www.youtube.com/watch?v=DOIpsOagVXU
  
- speaker: Paul Pu Liang
  institution: Carnegie Mellon University
  type: previous
  date: 9/1/2022
  title: Fundamentals of Multimodal Representation Learning - Towards Generalization and Quantification
  abstract: In recent years, the quest for artificial intelligence capable of digital, physical, and social intelligence has led to an explosion of interest in multimodal datasets and algorithms. This research area of multimodal machine learning studies the computational and theoretical foundations of learning from heterogeneous data sources. This talk studies two core challenges in multimodal learning- (1) constructing multimodal models and datasets that enable generalization across many modalities and different tasks, and (2) designing quantification methods to comprehensively understand the internal mechanics of multimodal representations and gain insights for safe real-world deployment. <br> In the first part, we study generalization in multimodal learning. Generalization is particularly beneficial when one modality has limited resources such as the lack of annotated data, noisy inputs, or unreliable labels, and presents a step towards processing many diverse and understudied modalities. To enable the study of generalization, we introduce MultiBench, a unified large-scale benchmark across a wide range of modalities, tasks, and research areas. Using MultiBench, we study generalization with parallel modalities, as well as in non-parallel scenarios, where we are presented with many modalities, but each task is defined only over a small subset of them. <br> The second part studies quantification of multimodal models via MultiViz, our recent attempt at a framework to understand the internal modeling of multimodal information and cross-modal interactions. We conclude this talk by discussing how future work can leverage these ideas to drive progress towards more general, scalable, and explainable multimodal models.
  bio: Paul Liang is a Ph.D. student in Machine Learning at CMU, advised by Louis-Philippe Morency and Ruslan Salakhutdinov. His research lies in the foundations of multimodal machine learning with applications in socially intelligent AI, understanding human and machine intelligence, natural language processing, healthcare, and education. His research is generously supported by a Facebook PhD Fellowship and a Center for Machine Learning and Health Fellowship, and has been recognized by awards at the NeurIPS 2019 workshop on federated learning and ICMI 2017. He regularly organizes courses, workshops, and tutorials on multimodal learning and was a workflow chair for ICML 2019. Website at <a href="https://www.cs.cmu.edu/~pliang/">https://www.cs.cmu.edu/~pliang/</a>
  recording: https://www.youtube.com/watch?v=ybJXOtJNpNw
  
- speaker: Arjun Desai
  institution: Stanford University
  type: previous 
  date: 9/8/2022
  title: Leveraging Physics-Based Priors for Label-Efficient, Robust MRI Reconstruction
  abstract: Deep learning has enabled improved image quality and fast inference times for various inverse problems, including accelerated MRI reconstruction. However, these models require access to large amounts of fully-sampled (labeled) data and are sensitive to clinically-pervasive distribution drifts. To tackle this challenge, we propose a family of consistency-based training strategies, which leverage physics-driven data augmentations and our domain knowledge of MRI physics to improve label efficiency and robustness to relevant distribution shifts. In this talk, we will discuss how two of these methods, Noise2Recon and VORTEX, can reduce the need for labeled data by over 10-fold and increase robustness to both physics-driven perturbations and variations in anatomy and MRI sequences & contrasts. We will also discuss how these techniques can simplify composing heterogenous augmentation and self-supervised methods into a unified framework.
  bio: Arjun is a 4th-year PhD student in Electrical Engineering working with Akshay Chaudhari and Chris Ré. He is broadly interested in how we can accelerate the pace at which artificial intelligence can be used safely and at scale in healthcare. His interests lie at the intersection of signal processing and machine learning, including representation learning for multimodal data, developing data-efficient & robust machine learning methods, and designing scalable clinical deployment and validation systems for medical image acquisition and analysis. Prior to Stanford, he received his B.E. in Biomedical Engineering and Computer Science from Duke University.
  recording: https://www.youtube.com/watch?v=acMtMT_iO4s
  
- speaker: Natalie Dullerud
  institution: Stanford University (prev. at University of Toronto)
  type: previous
  date: 9/15/2022
  title: Fairness in representation learning - a study in evaluating and addressing fairness via subgroup disparities in deep metric learning
  abstract: Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this talk, we will discuss evaluation of state-of-the-art DML methods trained on imbalanced data, and show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In the talk, we will first define fairness in DML through an analysis of three properties of the representation space -- inter-class alignment, intra-class alignment, and uniformity -- and propose finDML, the fairness in non-balanced DML benchmark to characterize representation fairness. Utilizing finDML, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (PARADE) to de-correlate feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics. In addition to covering salient aspects of fairness in deep metric learning, the talk will encompass a larger discussion of fairness metrics in representation learning at large, where our proposed definitions exist within representation learning, and how use of such metrics may vary based on domain.
  bio: Natalie Dullerud is an incoming PhD student at Stanford University and recently received her Masters from University of Toronto. She previously graduated with a Bachelor’s degree in mathematics from University of Southern California, with minors in computer science and chemistry. At University of Toronto, Natalie was awarded a Junior Fellowship at Massey College, and she has completed several research internships at Microsoft Research. Natalie’s research largely focuses on machine learning through differential privacy, algorithmic fairness, and applications to clinical and biological settings.
  recording: https://www.youtube.com/watch?v=82uoN0muHto
  
- speaker: No Session - Fall Quarter Break
  type: previous
  date: 9/22/2022
  
- speaker: Ethan Steinberg
  institution: Stanford University
  type: previous
  date: 9/29/2022
  title: Language Models Are An Effective Representation Learning Technique For Electronic Health Record Data
  abstract: The widespread adoption of electronic health records (EHRs) has fueled the development of using machine learning to build prediction models for various clinical outcomes. However, this process is often constrained by having a relatively small number of patient records for training the model. We demonstrate that using patient representation schemes inspired from techniques in natural language processing can increase the accuracy of clinical prediction models by transferring information learned from the entire patient population to the task of training a specific model, where only a subset of the population is relevant. Such patient representation schemes enable a 3.5% mean improvement in AUROC on five prediction tasks compared to standard baselines, with the average improvement rising to 19% when only a small number of patient records are available for training the clinical prediction model. <a href="https://www.sciencedirect.com/science/article/pii/S1532046420302653?via%3Dihub">Paper Linked Here</a>
  bio: Ethan Steinberg is a Ph.D. student in Computer Science at Stanford University, co-advised by Jure Leskovec and Nigam Shah. He is interested in the intersection of machine learning and healthcare, and how we can better use deep learning models to improve our healthcare system.
  recording: https://www.youtube.com/watch?v=qIFZQjiUGfQ
  
- speaker: Louis Blankemeier
  institution: Stanford University
  type: previous
  date: 10/6/2022
  title: Opportunistic Incidence Prediction of Multiple Chronic Diseases from Abdominal CT Imaging Using Multi-Task Learning
  abstract: Opportunistic computed tomography (CT) analysis is a paradigm where CT scans that have already been acquired for routine clinical questions are reanalyzed for disease prognostication, typically aided by machine learning. While such techniques for opportunistic use of abdominal CT scans have been implemented for assessing the risk of a handful of individual disorders, their prognostic power in simultaneously assessing multiple chronic disorders has not yet been evaluated. In this retrospective study of 9,154 patients, we demonstrate that we can effectively assess 5-year incidence of chronic kidney disease (CKD), diabetes mellitus (DM), hypertension (HT), ischemic heart disease (IHD), and osteoporosis (OST) using single already-acquired abdominal CT scans. We demonstrate that a shared multi-planar CT input, consisting of an axial CT slice occurring at the L3 vertebral level, as well as carefully selected sagittal and coronal slices, enables accurate future disease incidence prediction. Furthermore, we demonstrate that casting this shared CT input into a multi-task approach is particularly valuable in the low-label regime. With just 10% of labels for our diseases of interest, we recover nearly 99% of fully supervised AUROC performance, representing an improvement over single-task learning.
  bio: Louis Blankemeier is a PhD student in electrical engineering at Stanford in the machine intelligence in medical imaging group led by Professor Akshay Chaudhari. His research focuses on reusing the vast amounts of patient data, often collected to answer specific clinical questions, to automatically screen for unrelated indications, a paradigm referred to as opportunistic screening. He has also spent time working on medical AI in industry at GE Healthcare and Microsoft Health AI. He holds a master’s in electrical engineering from Stanford and bachelor’s degrees in physics and electrical engineering from the University of Southern California. 
  
- speaker: Break - <a href="https://docs.google.com/forms/d/e/1FAIpQLScLGUTBARtbI5Gzr9v1NnO_0gtejECvz9GI2Eb5NKe9N9FjRA/viewform">Speaker Nominations + Feedback Forms</a>
  type: previous
  date: 10/13/2022
  
- speaker: Break - <a href="https://docs.google.com/forms/d/e/1FAIpQLScLGUTBARtbI5Gzr9v1NnO_0gtejECvz9GI2Eb5NKe9N9FjRA/viewform">Speaker Nominations + Feedback Forms</a>
  type: previous
  date: 10/20/2022

- speaker: David Ouyang
  institution: Cedars-Sinai Medical Center
  type: previous
  date: 10/27/2022
  title: Development to Deployment of Cardiovascular AI
  abstract: Computer vision has advanced tremendously over the last decade, with performance of deep learning algorithms surpassing previous paradigms of image identification and segmentation. Cardiovascular care relies on precise detection, interpretation, and measurement of cardiovascular imaging, and could benefit from automation. In this talk, I will describe opportunities to explore cardiovascular medicine and its current challenges through the application of AI models and eventual assessment in clinical trials and deployment in clinical care.
  bio: David is a cardiologist and researcher in the Department of Cardiology and Division of Artificial Intelligence in Medicine at Cedars-Sinai Medical Center. As a physician-scientist and statistician with focus on cardiology and cardiovascular imaging, he works on applications of deep learning, computer vision, and the statistical analysis of large datasets within cardiovascular medicine. As a clinical echocardiographer, he works on applying deep learning for precision phenotyping in cardiac ultrasound. Additionally, he is interested in multi-modal datasets, linking ECR, echo, and MRI data for a holistic look at cardiovascular disease. He majored in statistics at Rice University, obtained his MD at UCSF, and received post-graduate medical education in internal medicine, cardiology, and a postdoc in computer science and biomedical data science at Stanford University.
  recording: https://youtu.be/73UnaLgjUNk
  
- speaker: Christian Bluethgen & Pierre Chambon
  institution: Stanford University
  type: previous
  date: 11/3/2022
  title: Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains
  abstract: Multi-modal foundation models are typically trained on millions of pairs of natural images and text captions. Although such models depict excellent generative capabilities, they do not typically generalize well to specific domains such as medical images that have fundamentally shifted distributions compared to natural images. Building generative models for medical images that faithfully depict clinical context may help alleviate the paucity of healthcare datasets. To investigate the capacity of a large pretrained latent diffusion model (Stable Diffusion) to generate medical domain-specific images, we explored the main components of the Stable Diffusion pipeline (the variational autoencoder, the U-Net and the text-encoder), to fine-tune the model to generate chest x-rays and evaluate the results on quantitative and qualitative levels. Our best-performing model can be text-conditioned to insert realistic-looking abnormalities like pleural effusions on synthetic radiological images, while maintaining a high accuracy on a classifier trained to detect the abnormality on real images. 
  bio: <b>Christian</b> is a physician-scientist, a radiologist with a clinical focus on thoracic imaging and currently a postdoctoral research fellow at the Stanford Center for Artificial Intelligence in Medicine and Imaging (AIMI). At the AIMI Center, he works on the application of deep learning for the detection, diagnosis and monitoring of interstitial lung disease and other thoracic pathologies, using large multi-modal imaging and EHR datasets. Before joining AIMI, he worked as a radiologist at the University Hospital Zurich, where he created computational simulations to visualize lung ultrasound wave propagation, developed NLP models for the classification of radiology reports, used radiomics for the differentiation of thymic neoplasms and deep learning for fracture detection and localization. In Zurich, he initiated the seminar series “Applied Machine Learning in Diagnostic Imaging” (currently in its fifth year), which brings together radiologists, researchers and clinicians from other medical specialties, and industry. <br> <b>Pierre</b>  is an ML researcher, who recently graduated from a master at Stanford ICME and will soon graduate from a master in France at Ecole Centrale Paris, with a focus on mathematical and computational methods for machine learning and deep learning. He has been involved at the AIMI center for the last two years, where he developed NLP methods for domain-specific applications to radiology as well as multimodal tasks along vision models. As part of the MIDRC initiative, he worked on classification tasks in the data- and compute-constraint settings, as well as a text de-identification tool useful for the broad sharing of medical notes within and between institutions. More recently, he tackled both image-to-text and text-to-image tasks, leading to models that can generate synthetic radiology images and reports, hopefully further useful to other machine learning applications in radiology.
  recording: https://www.youtube.com/watch?v=pSZDT5_EQj0

- speaker: Adriel Saporta
  institution: Courant Institute at NYU (New York University)
  type: previous
  date: 11/10/2022
  title: Benchmarking saliency methods for chest X-ray interpretation
  abstract: Saliency methods, which produce heat maps that highlight the areas of the medical image that influence model prediction, are often presented to clinicians as an aid in diagnostic decision-making. However, rigorous investigation of the accuracy and reliability of these strategies is necessary before they are integrated into the clinical setting. In this work, we quantitatively evaluate seven saliency methods, including Grad-CAM, across multiple neural network architectures using two evaluation metrics. We establish the first human benchmark for chest X-ray segmentation in a multilabel classification set-up, and examine under what clinical conditions saliency maps might be more prone to failure in localizing important pathologies compared with a human expert benchmark. We find that (1) while Grad-CAM generally localized pathologies better than the other evaluated saliency methods, all seven performed significantly worse compared with the human benchmark, (2) the gap in localization performance between Grad-CAM and the human benchmark was largest for pathologies that were smaller in size and had shapes that were more complex, and (3) model confidence was positively correlated with Grad-CAM localization performance. Our work demonstrates that several important limitations of saliency methods must be addressed before we can rely on them for deep learning explainability in medical imaging.
  bio: Adriel Saporta is a PhD candidate in Computer Science at the Courant Institute at NYU, where she is advised by Professor Rajesh Ranganath and is a DeepMind Scholar. Her research interests are at the intersection of AI and health, and she co-hosts The AI Health Podcast with Harvard's Professor Pranav Rajpurkar. Previously, Adriel conducted research on Apple’s Health AI team and in Dr. Andrew Ng’s Stanford Machine Learning Group. She has held engineering and product roles across both big tech (Apple, Amazon) and start-ups (SeatGeek, Common). She holds an MBA from the Stanford Graduate School of Business, an MS in Computer Science from Stanford University, and a BA in Comparative Literature from Yale University. Born and raised in Brooklyn, Adriel is half-Cuban and half-Greek.
  recording: https://youtu.be/-chA2LIZPIA

  
- speaker: Ramprasaath Selvaraju
  institution: Artera
  type: previous
  date: 11/17/2022
  title: Explaining Model Decisions and Fixing Them through Human Feedback
  abstract: In this talk, I will focus on how we can build algorithms that provide explanations for decisions emanating from deep networks in order to build user trust, incorporate domain knowledge into AI, learn grounded representations, and correct for any unwanted biases learned by our AI models.
  bio: Ramprasaath is a Sr. Machine Learning Scientist at Artera. Prior to this he was a Sr. Research Scientist at Salesforce. He did his Ph.D in Computer Science at Georgia Institute of Technology advised by Devi Parikh. He works at the intersection of machine learning, computer vision & language, explainable AI and more recently medical AI. He has held visiting positions at Brown University, Oxford University, Meta, Samsung, Microsoft and Tesla Autopilot. He has a Bachelor’s degree in Electrical and Electronics Engineering and a Masters in Physics from BITS-Pilani.
  recording: https://youtu.be/WUUS-34TZtI
  
- speaker: Thanksgiving Break (No MedAI Session)
  type: previous
  date: 11/25/2022
 
  
- speaker: Julian Acosta
  institution: Yale University
  type: upcoming
  date: 12/1/2022
  title: Multimodal Biomedical Artificial Intelligence - challenges and opportunities
  abstract: The increasing availability of biomedical data from large biobanks, electronic health records, medical imaging, wearable and ambient biosensors, and the lower cost of genome and microbiome sequencing have set the stage for the development of multimodal artificial intelligence solutions that capture the complexity of human health and disease. In this talk, we will discuss key applications enabled by multimodal AI in health, along with the challenges we need to overcome to achieve its potential.
  bio: Julian N. Acosta, MD, trained as a neurologist at the Fleni institute in Argentina before joining Yale University as a postdoctoral fellow in 2019, where his research focused on population genetics and advanced neuroimaging in neurovascular disease. He has also collaborated with Dr. Rajpurkar's Medical AI lab on multiple projects related to the application of artificial intelligence in healthcare, and he currently works as a Clinical Data Scientist at Rad AI Inc.
  
- speaker: Ali Mottagi
  institution: Stanford University
  type: upcoming
  date: 12/8/2022
  
- speaker: Roxana Daneshjou 
  institution: Stanford University
  type: upcoming
  date: 1/26/2023
  
  
  

  

 
  

 

  
  
  


  
  

  
  
  

