# - speaker:
#   type: upcoming (or previous)
#   date:
#   title:
#   abstract:
#   bio:
#   livestream:
#   recording:

#### Previous talks: make sure order is right. Can just replace upcoming with previous in place

- speaker: Amirata Ghorbani
  institution: Stanford University
  type: previous
  date: 4/1/21
  title: Equitable Valuation of Data
  abstract: As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this talk, we discuss a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on a number of data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We introduce Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. We then briefly discuss the notion of distributional Shapley, where the value of a point is defined in the context of underlying data distribution.
  bio: <a href="https://www.amiratag.com/" target="_blank">Amirata Ghorbani</a> is a fifth year PhD student at Stanford University advised by James Zou. He primarily works on different problems in machine learning such as research on equitable methods for data valuation, algorithms to interpret machine learning models, ways to make existing ML predictors fairer, and creating ML systems for healthcare applications such as cardiology and dermatology. He has also worked as a research intern in Google Brain, Google Brain Medical, and Salesforce Research.
  recording: https://youtu.be/TzbFPsJ3o7U

- speaker: Michael Zhang
  institution: Stanford University
  type: previous
  date: 4/8/21
  title: Federated Learning with FOMO for Personalized Training and Deployment
  abstract: Federated learning (FL) is an exciting and relatively new deep learning framework that canonically trains a single global model across decentralized local datasets maintained by participating clients. Accordingly with respect to making deep learning more deployable, FL is particularly promising in real world settings where technological or privacy constraints prevent individual data from being aggregated together. However, one model may not always be optimal for all participating clients. From healthcare to recommendation systems, we would ideally like to learn and deliver a personalized model for each participating client, as data may not be identically distributed from one client to another. This problem is emphasized when we consider how we might deploy FL in practice, where individual clients may only choose to federate if they can guarantee a benefit from the model produced at the end.<br><br>In this talk, I will present some recent work on one solution, called FedFomo, where each client effectively only federates with other relevant clients to obtain stronger personalization. First we will review federated learning as a machine learning framework, emphasizing the motivations behind personalized FL. I will then go into the origin story of FedFomo's name, highlighting a simple yet effective approach based both on the "fear of missing out" and "first order model optimization". In tandem, these ideas describe how FedFomo can efficiently figure out how much each client can benefit from another's locally trained model, and then use these values to calculate optimal federated models for each client. Critically, this does not assume knowledge of any underlying data distributions or client similarities, as this information is often not known apriori. Finally, I will describe recent empirical results on FedFomo's promising performance on a variety of federated settings, datasets, and degrees of local data heterogeneity, leading to wider discussion on the future directions and impact of federated learning and distributed machine learning, when personalization is in the picture.
  bio: Michael Zhang is a Computer Science PhD Student at Stanford, currently working with Chris Ré and Chelsea Finn. He is broadly interested in making machine learning more deployable and reliable in the "real world", especially through the lenses of improving model robustness and personalization to distribution shifts and new tasks, as well as developing new systems that enable collaborative machine learning and/or learning with less labels.
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  recording: https://www.youtube.com/watch?v=svBDaT4IY4A

- speaker: Jason Fries
  institution: Stanford University
  type: previous
  date: 4/15/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Weakly Supervised Learning in Medicine (Better Living through Programmatic Supervision)
  abstract: The high cost of building labeled training sets is one of the largest barriers to using supervised machine learning in medicine. Privacy concerns create additional challenges to sharing training data for modalities like patient notes, making it difficult to train state-of-the-art NLP tools for analyzing electronic health records. The COVID-19 pandemic underscores the need for faster, more systematic methods of curating and sharing training data. One promising approach is weakly supervised learning, where low cost and often noisy label sources are combined to programmatically generate labeled training data for commodity deep learning architectures such as BERT. Programmatic labeling takes a data-centric view of machine learning and provides many of the same practical benefits as software development, including better consistency, inspectability, and creating higher-level abstractions for experts to inject domain knowledge into machine learning models.<br><br>In this talk I outline our new framework for weakly supervised clinical entity recognition, Trove, which builds training data by combining multiple public medical ontologies and other imperfect label sources. Instead of manually labeling data, in Trove annotators focus on defining labelers using ontology-based properties like semantic types as well as optional task-specific rules. On four named entity benchmark tasks, Trove approaches the performance of models trained using hand-labeled data. However unlike hand-labeled data, our labelers can be shared and modified without compromising patient privacy.
  bio: Jason Fries (<a href="http://web.stanford.edu/~jfries/" target="_blank">http://web.stanford.edu/~jfries/</a>) is a Research Scientist at Stanford University working with Professor Nigam Shah at the Center for Biomedical Informatics Research. He previously completed his postdoc with Professors Chris Ré and Scott Delp as part of Stanford's Mobilize Center. He received his PhD in computer science from the University of Iowa, where he studied computational epidemiology and NLP methods for syndromic surveillance. His recent research explores weakly supervised and few-shot learning in medicine, with a focus on methods for incorporating domain knowledge into the training of machine learning models. 
  recording: https://www.youtube.com/watch?v=EcvLqmeD7SE

- speaker: Pradeeban Kathiravelu
  institution: Emory University
  type: previous
  date: 4/22/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Understanding Scanner Utilization with Real-Time DICOM Metadata Extraction
  abstract: Understanding system performance metrics ensures better utilization of the radiology resources with more targeted interventions. The images produced by radiology scanners typically follow the DICOM (Digital Imaging and Communications in Medicine) standard format. The DICOM images consist of textual metadata that can be used to calculate key timing parameters, such as the exact study durations and scanner utilization. However, hospital networks lack the resources and capabilities to extract the metadata from the images quickly and automatically compute the scanner utilization properties. Thus, they resort to using data records from the Radiology Information Systems (RIS). However, data acquired from RIS are prone to human errors, rendering many derived key performance metrics inadequate and inaccurate. Hence, there is motivation to establish a real-time image transfer from the Picture Archiving and Communication Systems (PACS) to receive the DICOM images from the scanners to research clusters to conduct such metadata processing to evaluate scanner utilization metrics efficiently and quickly.<br><br>In this talk, we present Niffler (https://github.com/Emory-HITI/Niffler), an open-source DICOM Framework for Machine Learning Pipelines and Processing Workflows. Niffler analyzes the scanners' utilization as a real-time monitoring framework that retrieves radiology images into a research cluster using the DICOM networking protocol and then extracts and processes the metadata from the images. Niffler facilitates a better understanding of scanner utilization across a vast healthcare network by observing properties such as study duration, the interval between the encounters, and the series count of studies. Benchmarks against using the RIS data indicate that our proposed framework based on real-time PACS data estimates the scanner utilization more accurately. Our framework has been running stable and supporting several machine learning workflows for more than two years on our extensive healthcare network in pseudo-real-time. We further present how we use the Niffler framework for real-time and on-demand execution of machine learning (ML) pipelines on radiology images.
  bio: Pradeeban Kathiravelu is a postdoctoral researcher at the Department of Biomedical Informatics in Emory University. He has an Erasmus Mundus Joint Doctorate in Distributed Computing from Universidade de Lisboa (Lisbon, Portugal) and Université catholique de Louvain (Louvain-la-Neuve, Belgium). His research focus includes researching and developing latency-aware Software-Defined Systems and cloud-assisted networks for radiology workflows at the edge.
  recording: https://youtu.be/oiQAjfYc9Sc


- speaker: Joseph Cohen
  institution: Stanford University
  type: previous
  date: 4/29/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Gifsplanation via Latent Shift - A Simple Autoencoder Approach to Counterfactual Generation for Chest X-rays
  abstract: <strong>Motivation</strong> Traditional image attribution methods struggle to satisfactorily explain predictions of neural networks. Prediction explanation is important, especially in medical imaging, for avoiding the unintended consequences of deploying AI systems when false positive predictions can impact patient care. Thus, there is a pressing need to develop improved models for model explainability and introspection. <br><strong>Specific problem</strong> A new approach is to transform input images to increase or decrease features which cause the prediction. However, current approaches are difficult to implement as they are monolithic or rely on GANs. These hurdles prevent wide adoption.<br><strong>Our approach</strong> Given an arbitrary classifier, we propose a simple autoencoder and gradient update (Latent Shift) that can transform the latent representation of a specific input image to exaggerate or curtail the features used for prediction. We use this method to study chest X-ray classifiers and evaluate their performance. We conduct a reader study with two radiologists assessing 240 chest X-ray predictions to identify which ones are false positives (half are) using traditional attribution maps or our proposed method. This work will be presented at MIDL 2021.<br><strong>Results</strong> We found low overlap with ground truth pathology masks for models with reasonably high accuracy. However, the results from our reader study indicate that these models are generally looking at the correct features.We also found that the Latent Shift explanation allows a user to have more confidence in true positive predictions compared to traditional approaches (0.15±0.95 in a 5 point scale with p=0.01) with only a small increase in false positive predictions (0.04±1.06 with p=0.57).<br><strong>Project Page</strong> https://mlmed.org/gifsplanation/ <br><strong>Source code</strong> https://github.com/mlmed/gifsplanation
  bio: Joseph Paul Cohen is a researcher and pragmatic engineer. He currently focuses on the challenges in deploying AI tools in medicine specifically computer vision and genomics and is affiliated to Stanford AIMI. He maintains many open source projects including Chester the AI radiology assistant, TorchXRayVision, and BlindTool – a mobile vision aid app. He is the director of the Institute for Reproducible Research, a US non-profit which operates ShortScience.org and Academic Torrents.
  recording: https://youtu.be/2dHY5IJTbKM
  
- speaker: Angshuman Paul
  institution: NIH
  type: previous
  date: 5/6/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Few-shot Chest X-ray Diagnosis Using Clinical Images and the Images from the Published Scientific Literature
  abstract: Few-shot learning is the art of machine learning that tries to mimic the human cognitive ability of understanding new object classes from a few labeled training examples. In the last few years, several few-shot learning methods have been proposed for different tasks related to natural images. However, few-shot learning is relatively unexplored in the field radiology image analysis. In this seminar, we will present two few-shot learning methods for chest x-ray diagnosis. Our first method uses a discriminative ensemble trained using labeled clinical chest x-ray images. The second method uses labeled chest x-ray images from the published scientific literature and unlabeled clinical chest x-ray images to train a machine learning model. Experiments show the superiority of the proposed methods over several existing few-shot learning methods.
  bio: Angshuman Paul (M.E., Ph.D.) is a visiting (postdoctoral) fellow at the National Institutes of Health, USA. His primary research interest is in Machine Learning, Medical Imaging, and Computer Vision. He earned his Ph.D. from the Indian Statistical Institute, India. He has held a visiting scientist position at the Indian statistical Institute (2019) and a graduate intern position at the University of Missouri-Columbia (2011). Dr. Paul is the recipient of the NIH Intramural Fellowship (2019) from the National Institutes of Health, USA, and the best paper award in the Tenth Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP, 2016). He serves as a reviewer of several journals including IEEE Transactions on Medical Imaging, Pattern Recognition Letters, and IEEE Transactions on Image Processing.
  recording: https://www.youtube.com/watch?v=TtXrowIpXPM

- speaker: Xiaoyuan Guo
  institution: Emory University
  type: previous
  date: 5/13/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Segmentation and Quantification of Breast Arterial Calcifications (BAC) on Mammograms
  abstract: Measurements of breast arterial calcifications (BAC) can offer a personalized, noninvasive approach to risk-stratify women for cardiovascular disease such as heart attack and stroke. We aim to detect and segment breast arterial calcifications in mammograms accurately and suggest novel measurements to quantify detected BAC for future clinical applications. To separate BAC in mammograms, we propose a lightweight fine vessel segmentation method Simple Context U-Net (SCU-Net). To further quantify calcifications, we test five quantitative metrics to inspect the progression of BAC for subjects- Sum of Mask Probability Metric (PM), Sum of Mask Area Metric (AM), Sum of Mask Intensity Metric (SIM), Sum of Mask Area with Threshold Intensity Metric (TAMx) and Sum of Mask Intensity with Threshold X Metric (TSIMx). Finally, we demonstrate the ability of the metrics to longitudinally measure calcifications in a group of 26 subjects and evaluate our quantification metrics compared to calcified voxels and calcium mass on breast CT for 10 subjects.
  bio: Xiaoyuan Guo is a Computer Science PhD student at Emory University, working with Prof. Imon Banerjee, Prof. Hari Trivedi and Prof. Judy Wawira Gichoya. Her primary research interests are computer vision and medical image processing, especially improving medical image segmentation, classification, object detection accuracy with mainstream computer vision techniques. She is also interested in solving open-world medical tasks.
  recording: https://www.youtube.com/watch?v=CWuBD9XTN7s

- speaker: Nandita Bhaskhar
  institution: Stanford University
  type: previous
  date: 5/20/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Self-supervision & Contrastive Frameworks -- a vision-based review
  abstract: Self-supervised representation learning and contrastive techniques have picked up a lot interest in the last couple of years, especially in computer vision. Until recently, deep learning's successes thus far have been associated with a supervised learning paradigm, wherein labelled datasets are used to train models on specific tasks. This need for labelled datasets has been identified as the bottleneck for scaling deep learning models across various tasks and domains. They rely heavily on costly, time-consuming dataset curation and labelling schemes. <br><br>Self-supervision allows us to learn representations from large unlabelled datasets. Instead of relying on labels for inputs, it depends on designing suitable pre-text tasks to generate pseudo-labels from the data directly. Contrastive learning refers to a special subset of these self-supervised methods that have achieved the most success recently. In this talk, I will go over the top 6 recent frameworks - SimCLR, MoCo V2, BYOL, SwAV, DINO and Barlow Twins, giving a deeper dive into their methodology & performance and comparing each of the frameworks' strengths and weaknesses and discuss their suitability for applications in the medical domain.
  bio: Nandita Bhaskhar (see <a href="http://web.stanford.edu/~nanbhas/" target="_blank">website</a>) is a PhD student in the Department of Electrical Engineering at Stanford University advised by Daniel Rubin. She received her B.Tech in Electronics Engineering from the Indian Institute of Information Technology, IIIT, with the highest honours. She is broadly interested in developing machine learning methodology for medical applications. Her current research focuses on observational supervision and self-supervision for leveraging unlabelled medical data and out-of-distribution detection for reliable clinical deployment. Outside of research, her curiosity lies in a wide gamut of things including but not restricted to biking, social dance, travelling, creative writing, music, getting lost, hiking and exploring new things.
  recording: https://youtu.be/OF_7dBbb_N0

- speaker: Amara Tariq
  institution: Emory University
  type: previous
  date: 5/27/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Patient-specific COVID-19 Resource Utilization Prediction Using Fusion AI model
  abstract: Strain on healthcare resources brought forth by recent COVID-19 pandemic has highlighted the need for efficient resource planning and allocation through prediction of future consumption. Machine learning can predict resource utilization such as the need for hospitalization based on past medical data stored in electronic medical records (EMR). We experimented with fusion modeling to develop patient-specific clinical event prediction model based on patient’s medical history and current medical indicators. A review of feature importance provides insight for future research and feedback from the community on the significance of various predictors of COVID-19 disease trajectory.
  bio: Dr. Amara Tariq received her PhD degree in Computer Science from University of Central Florida in 2016 where she was Fulbright Scholar. Her research was focused on automatic understanding of cross-modal semantic relationships, especially relations between images and text. After earning her PhD, she designed and taught courses focused on Artificial Intelligence and Machine Learning at graduate and post-graduate level in her home country, i.e., Pakistan. Her research interests evolved to include multi-modal data related to the fields of bioinformatics and health science. Since the beginning of 2020, she has been working in post-doctoral research capacity at Bioinformatics department, Emory University, GA. At Emory University, her research has been focused on analyzing electronic medical records, imaging studies, and clinical reports and notes for intelligent decision making regarding disease management and healthcare resource optimization. Her research has resulted in publications in top-tier venues including IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), IEEE Transactions on Image Processing (TIP), IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), Journal of American College of Radiology (JACR), and npj Digital Medicine.
  recording: https://www.youtube.com/watch?v=E4TuOcY89E8
  
- speaker: No session this week -- summer break! 
  type: previous
  date: 6/3/21
  
- speaker: Edward Choi
  institution: KAIST
  type: previous
  date: 6/10/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Learning the Structure of EHR with Graph Convolutional Transformer
  abstract: Large-scale electronic health records (EHR) provide a great opportunity for learning representation of clinical entities (such as codes, visits, patients). As EHR data are typically stored in a relational database, their diverse information (diagnosis, medications, etc) can be naturally viewed as a graph. In this talk, we will study how this graphical structure can be exploited, or even learned for supervised prediction tasks using a combination of graph convolution and self-attention. Additionally, we will briefly present more recent works regarding multi-modal learning using Transformers.
  bio: Edward Choi is currently an assistant professor at KAIST, South Korea. He received his PhD at Georgia Tech under the supervision of Professor Jimeng Sun, focusing on interpretable deep learning methods for longitudinal electronic health records. Before he joined KAIST, Ed was a software engineer at Google Health Research, developing deep learning models for predictive healthcare. His current research interests include machine learning, healthcare analytics, and natural language processing.
  recording: https://www.youtube.com/watch?v=IZXcuWFIBcY&t=18s

- speaker: Florian Dubost
  institution: Stanford University
  type: previous
  date: 6/17/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Hydranet -- Data Augmentation for Regression Neural Networks
  abstract: Deep learning techniques are often criticized to heavily depend on a large quantity of labeled data. This problem is even more challenging in medical image analysis where the annotator expertise is often scarce. We propose a novel data-augmentation method to regularize neural network regressors that learn from a single global label per image. The principle of the method is to create new samples by recombining existing ones. We demonstrate the performance of our algorithm on two tasks- estimation of the number of enlarged perivascular spaces in the basal ganglia, and estimation of white matter hyperintensities volume. We show that the proposed method improves the performance over more basic data augmentation. The proposed method reached an intraclass correlation coefficient between ground truth and network predictions of 0.73 on the first task and 0.84 on the second task, only using between 25 and 30 scans with a single global label per scan for training. With the same number of training scans, more conventional data augmentation methods could only reach intraclass correlation coefficients of 0.68 on the first task, and 0.79 on the second task.
  bio: Florian Dubost is a postdoctoral researcher in biomedical data science at Stanford University, CA, USA, and has with six years of experience in machine learning. He holds a PhD in medical computer vision and reached top rankings in international deep learning competitions. He is member of program committees at conference workshops in AI and medicine, authored a book in AI and neurology, and is an author and reviewer for top international journals and conferences in AI and medicine with over 20 published articles, including 11 as first author.
  recording: https://www.youtube.com/watch?v=ph6xEFAu9A0
  
- speaker: Garrett Honke
  institution: Google X, the Moonshot Factory
  type: previous
  date: 6/24/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: βVAE Representation Learning and Explainability for Psychopathology with EEG and the constraints on deployment in the real world
  abstract: Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using β-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol-Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model.
  bio: Garrett is a neuroscientist working as a Senior Research Scientist at X, the Moonshot Factory (formerly Google X). He works with projects in the early pipeline on problem areas that generally involve ML, datascience, and human-emitted data.
  recording: https://www.youtube.com/watch?v=NU6YZVv0q2Y
  
- speaker: Rikiya Yamashita
  institution: Stanford University
  type: previous
  date: 7/1/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation
  abstract: Suboptimal generalization of machine learning models on unseen data is a key challenge which hampers the clinical applicability of such models to medical imaging. Although various methods such as domain adaptation and domain generalization have evolved to combat this challenge, learning robust and generalizable representations is core to medical image understanding, and continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation for histoPathology), a form of data augmentation based on random style transfer from non-medical style source such as artistic paintings, for learning domain-agnostic visual representations in computational pathology. Style transfer replaces the low-level texture content of an image with the uninformative style of randomly selected style source image, while preserving the original high-level semantic content. This improves robustness to domain shift and can be used as a simple yet powerful tool for learning domain-agnostic representations. We demonstrate that STRAP leads to state-of-the-art performance, particularly in the presence of domain shifts, on two particular classification tasks in computational pathology.
  bio: Rikiya Yamashita is radiologist turned applied research scientist working as a postdoctoral researcher in the Department of Biomedical Data Science at Stanford University. He is broadly interested in developing machine learning methodology for extracting knowledge from unstructured biomedical data. With his dual expertise, he is passionate about bridging the gap between machine learning and clinical medicine.
  recording: https://www.youtube.com/watch?v=WWlGOreDbzU
  
- speaker: Andre Esteva
  institution: Salesforce Research
  type: upcoming
  date: 7/8/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  title: Frontiers of Medical AI - Therapeutics and Workflows
  abstract: As the artificial intelligence and deep learning revolutions have swept over a number of industries, medicine has stood out as a prime area for beneficial innovation. The maturation of key areas of AI - computer vision, natural language processing, etc. - have led to their successive adoption in certain application areas of medicine. The field has seen thousands of researchers and companies begin pioneering new and creative ways of benefiting healthcare with AI. Here we'll discuss two vitally important areas - therapeutics, and workflows. In the space of therapeutics we'll discuss how multi-modal AI can support physicians in complex decision making for cancer treatments, and how natural language processing can be repurposed to create custom-generated proteins as potential therapeutics. Within workflows, we'll explore how to build a COVID-specialized search engine, and discuss ways in which this could empower health systems to securely, and accurately, search over their highly sensitive data. 
  bio: Andre Esteva is a researcher and entrepreneur in deep learning and computer vision. He currently serves as Head of Medical AI at Salesforce Research. Notably, he has led research efforts in AI-enabled medical diagnostics, and therapeutic decision making. His work has shown that computer vision algorithms can match and exceed the performance of top physicians at diagnosing cancers from medical imagery. Expanded into video they can diagnose behavioral conditions like autism. In the space of AI-enabled therapeutics, his research leverages multi-modal datasets to train AI models that can personalize oncology treatments for patients by determing the best course of therapy for them. He has worked at Google Research, Sandia National Labs, and GE Healthcare, and has co-founded two tech startups. He obtained his PhD in Artificial Intelligence at Stanford, where he worked with Sebastian Thrun, Jeff Dean, Stephen Boyd, Fei-Fei Li, and Eric Topol.
  recording: https://youtu.be/fSpwx8xJ4v8
  

- speaker: TBD
  type: upcoming
  date: 7/15/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  
- speaker: Liangqiong Qu
  institution: Stanford University
  type: upcoming
  date: 7/22/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  
- speaker: Shantanu Thakoor
  institution: DeepMind
  type: upcoming
  date: 7/29/21
  livestream: https://www.youtube.com/channel/UCOkkljs06NPPkjNysCdQV4w
  
  

